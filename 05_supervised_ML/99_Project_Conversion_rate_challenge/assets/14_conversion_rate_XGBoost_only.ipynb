{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score # confusion_matrix, \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler  , PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression \n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "k_target        = \"converted\"\n",
    "k_samples_ratio = 100/100   # percentage of observations to be taken into account. Pass 100/100 for final testing \n",
    "k_test_size     = 20/100    # see train_test_split\n",
    "k_random_state  = 42        # you know why...\n",
    "header          = \"conversion_data_test_predictions_\"\n",
    "author          = \"PHILIPPE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>new_user</th>\n",
       "      <th>source</th>\n",
       "      <th>total_pages_visited</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Ads</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Seo</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Direct</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  age  new_user  source  total_pages_visited  converted\n",
       "0    China   22         1  Direct                    2          0\n",
       "1       UK   21         1     Ads                    3          0\n",
       "2  Germany   20         0     Seo                   14          1\n",
       "3       US   23         1     Seo                    3          0\n",
       "4       US   28         1  Direct                    3          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./assets/conversion_data_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"shape : {df.shape}\\n\")\n",
    "# print()\n",
    "\n",
    "# display(df.describe(include=\"all\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.info(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of null val :\")\n",
    "# print(100 * df.isnull().sum() / df.shape[0])\n",
    "# # print (df.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Duplicates     : \", df.duplicated().sum())\n",
    "# print(\"Col duplicated : \", df.columns.duplicated() )\n",
    "\n",
    "# print()\n",
    "# print(\"Unique countries : \", df[\"country\"].unique())\n",
    "# print(\"Unique sources   : \", df[\"source\"].unique())\n",
    "\n",
    "# # print(df[\"col_name\"].value_counts())\n",
    "# # print(df.isnull().sum().sort_values(ascending=False).head(11))\n",
    "# df[k_target].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniquement sur les données de train\n",
    "# PAS sur les données de test (validation) ou les données de test jamais vues\n",
    "# ATTENTION : \n",
    "# Il est sans doute préférable de minorer l'influence des observations redontantes\n",
    "\n",
    "# indices_redondants = [0, 1, 5, 10]  # Exemple d'indices de données redondantes\n",
    "\n",
    "# # Calculer les poids des échantillons en inversant leur fréquence\n",
    "# # Pour pondérer à la baisse les échantillons redondants, vous pouvez diviser 1 par le nombre d'occurrences de chaque échantillon\n",
    "# weights = np.ones(len(y_train))\n",
    "# for idx in indices_redondants:\n",
    "#     weights[idx] = 1.0 / indices_redondants.count(idx)\n",
    "\n",
    "# Créer un objet DMatrix pour les données d'entraînement avec les poids des échantillons\n",
    "# dtrain = xgb.DMatrix(data=X_train, label=y_train, weight=weights)\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "# Réutiliser aussi les paramètres issues du GridSearchCV\n",
    "# params = {\n",
    "#     'objective': 'binary:logistic',  # pour un problème de classification binaire\n",
    "#     'eval_metric': 'auc'  # Métrique d'évaluation\n",
    "# }\n",
    "\n",
    "# # Entraîner le modèle XGBoost\n",
    "# model = xgb.train(params=params, dtrain=dtrain, num_boost_round=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def add_weight_col(df):\n",
    "  \n",
    "#   # print(f\"shape : {df.shape}\\n\")\n",
    "#   # df.drop_duplicates(inplace=True)\n",
    "#   # print(f\"shape : {df.shape}\\n\")\n",
    "\n",
    "#   # print(f\"shape : {df.shape}\")\n",
    "\n",
    "#   # Créer une colonne avec l poids des doublons\n",
    "#   # Supprimer les doublons\n",
    "#   df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "#   # Compter le nombre d'occurrences de chaque ligne dans le DataFrame d'origine\n",
    "#   counts = df.groupby(df.columns.tolist()).size().reset_index(name='poids')\n",
    "\n",
    "#   # Fusionner la colonne occurences avec le df sans doublons\n",
    "#   df = pd.merge(df_no_duplicates, counts, on=df.columns.tolist(), how='left')\n",
    "\n",
    "#   # print(f\"shape : {df.shape}\")\n",
    "#   # df.to_csv(\"./assets/pourbench_colab.csv\")\n",
    "#   return df  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "# Uniquement sur les données de train\n",
    "# PAS sur les données de test (validation) ou les données de test jamais vues\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# numeric_columns = df[[\"age\", \"total_pages_visited\"]]\n",
    "# print(type(numeric_columns))\n",
    "# # numeric_columns = [\"age\tnew_user\", \"total_pages_visited\"]\n",
    "\n",
    "# # 1. Calculez l'IQR pour chaque colonne numérique\n",
    "# Q1 = numeric_columns.quantile(0.25)\n",
    "# Q3 = numeric_columns.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# # 2. Calculez les limites supérieure et inférieure\n",
    "# lower_bound = Q1 - 1.5 * IQR\n",
    "# upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# # Créez un masque pour identifier les lignes avec des valeurs aberrantes dans les colonnes numériques\n",
    "# outliers_mask = ((numeric_columns < lower_bound) | (numeric_columns > upper_bound)).any(axis=1)\n",
    "\n",
    "# # Filtrez les lignes avec des valeurs aberrantes uniquement dans les colonnes numériques\n",
    "# df = df[~outliers_mask]\n",
    "# print(df.shape)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"shape : {df.shape}\")\n",
    "\n",
    "# # df = add_weight_col(df)  \n",
    "\n",
    "# print(f\"shape : {df.shape}\")\n",
    " \n",
    "\n",
    "# # On peut ici limiter la taille de df pour aller plus vite par exemple  \n",
    "# # df = df.sample(int(k_samples_ratio*len(df)))\n",
    "# # df = df.iloc[:int(k_samples_ratio*len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      "   country  age  new_user  source  total_pages_visited\n",
      "0    China   22         1  Direct                    2\n",
      "1       UK   21         1     Ads                    3\n",
      "2  Germany   20         0     Seo                   14\n",
      "3       US   23         1     Seo                    3\n",
      "4       US   28         1  Direct                    3\n",
      "(284580, 5)\n",
      "\n",
      "y :\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: converted, dtype: int64\n",
      "(227664, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split entre X et y\n",
    "X = df.drop(columns = k_target)\n",
    "y = df[k_target]\n",
    "\n",
    "print(\"X :\")\n",
    "print(X.head())\n",
    "print(X.shape)\n",
    "print()\n",
    "\n",
    "print(\"y :\")\n",
    "print(y.head())\n",
    "\n",
    "# Des dataframes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=k_test_size, random_state=k_random_state, stratify = y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'new_user', 'total_pages_visited'], dtype='object')\n",
      "Index(['country', 'source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numeric_features = X.select_dtypes(include=\"number\").columns\n",
    "print(numeric_features)\n",
    "\n",
    "categorical_features = X.select_dtypes(exclude=\"number\").columns\n",
    "print(categorical_features)\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "  steps=[\n",
    "    # (\"imputer_num\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler_num\", StandardScaler()),\n",
    "  ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "  steps=[\n",
    "      # (\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),  \n",
    "      # (\"imputer_cat\", SimpleImputer(fill_value=\"missing\", strategy=\"constant\")),  \n",
    "      (\"encoder_cat\", OneHotEncoder(drop=\"first\")),                 \n",
    "      # (\"encoder_cat\", OneHotEncoder(handle_unknown='ignore', sparse=False)),                 \n",
    "    ]\n",
    "  )\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "      (\"num\", numeric_transformer,     numeric_features),\n",
    "      (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.277  0.676 -0.262  0.     0.     1.     0.     0.   ]\n",
      " [-0.189  0.676 -0.561  0.     0.     0.     0.     0.   ]\n",
      " [ 0.657 -1.479 -0.561  0.     0.     1.     0.     1.   ]\n",
      " [-0.914  0.676  0.934  0.     0.     1.     0.     1.   ]\n",
      " [ 1.262  0.676 -0.561  0.     1.     0.     0.     0.   ]]\n",
      "(227664, 8)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# des nd array\n",
    "print(X_train[0:5].round(3))\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'xgb__gamma': 0, 'xgb__lambda': 1, 'xgb__learning_rate': 0.4, 'xgb__max_depth': 6, 'xgb__n_estimators': 600, 'xgb__objective': 'binary:logistic', 'xgb__reg_alpha': 0, 'xgb__subsample': 1}\n",
      "f1 \t\t precision \t recall\n",
      "0.751957 \t 0.840511 \t 0.680283\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'learning_rate': [0.1, 0.01],\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [3, 5],\n",
    "#     'subsample': [0.8, 0.9],\n",
    "#     'colsample_bytree': [0.8, 0.9],\n",
    "#     'scale_pos_weight': [30, 20, 10, 1] \n",
    "                                        \n",
    "# }\n",
    "\n",
    "# Tres très loong. J'ai arrêté aprs 441 min. A refaire une nuit\n",
    "# https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'n_estimators': [200, 400, 600],\n",
    "#     'max_depth': [6, 8, 10],\n",
    "#     'subsample': [0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "#     'scale_pos_weight': [10, 20, 30]  # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# }                                     # A typical value to consider: sum(negative instances) / sum(positive instances). \n",
    "\n",
    "\n",
    "# Composite estimators\n",
    "# https://scikit-learn.org/stable/modules/compose.html\n",
    "pipe = Pipeline(steps=[\n",
    "    # (\"poly\", PolynomialFeatures()),\n",
    "    (\"xgb\", XGBClassifier())\n",
    "])\n",
    "\n",
    "# param_grid = {\n",
    "#     \"poly__degree\"            : [1],\n",
    "#     'xgb__learning_rate'      : [0.1, 1],\n",
    "#     'xgb__n_estimators'       : [400, 600],\n",
    "#     #'xgb__max_depth'         : [6, 8],\n",
    "#     #'xgb__subsample'         : [0.8, 0.9],\n",
    "#     #'xgb__colsample_bytree'  : [0.8, 0.9],\n",
    "#    'xgb__scale_pos_weight'    : [10, 20]  # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# }                                         # A typical value to consider: sum(negative instances) / sum(positive instances). \n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/a-guide-to-xgboost-hyperparameters-87980c7f44a9\n",
    "param_grid = {\n",
    "    # \"poly__degree\"          : [1],\n",
    "    \"xgb__objective\"          : [\"binary:logistic\"],\n",
    "    'xgb__n_estimators'       : [600],        # default 100\n",
    "    'xgb__subsample'          : [1],            # default 1 [0,1]\n",
    "    'xgb__max_depth'          : [6],          # default 6\n",
    "    'xgb__learning_rate'      : [0.4],         # default 0.3\n",
    "    'xgb__gamma'               : [0],         # default 0 [0-1[ regulation\n",
    "    'xgb__reg_alpha'           : [0],         # default 0 L1 regulation\n",
    "    'xgb__lambda'               : [1],         # default 1 L2 regulation\n",
    "    # 'xgb__scale_pos_weight'   : [0.5 + i * 0.5 for i in range(20)],         # ici de 0.5 à 10. default 1 Neg / Pos instances Control the balance of positive and negative weights, useful for unbalanced classes\n",
    "\n",
    "\n",
    "\n",
    "    # 'xgb__colsample_bytree'   : [1],         # default 1 specify the fraction of columns to be subsampled.\n",
    "    # 'xgb__colsample_bylevel'   : [1],         # default 1 specify the fraction of columns for training at each level\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"f1\", verbose = 3, n_jobs = -1, cv = 5) #refit=True,     \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "print(f\"f1 \\t\\t precision \\t recall\")\n",
    "print(f\"{f1_score(y_test,  y_pred):.6f} \\t {precision_score(y_test,  y_pred):.6f} \\t {recall_score(y_test,  y_pred):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement sur l'ensemble du jeu de données \n",
    "* sans le diviser en train et test\n",
    "* L'idée c'est d'utiliser un max d'observations pour ajuster les paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Y a plus de train_test_split\n",
    "X = df.drop(columns = k_target)\n",
    "y = df[k_target]\n",
    "\n",
    "# print(X.shape)\n",
    "# print(type(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# print(X[0:5].round(3))\n",
    "# print(X.shape)\n",
    "# print(type(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = XGBClassifier()\n",
    "\n",
    "# clf.set_params(**best_params)\n",
    "# clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 \t\t precision \t recall\n",
      "0.783811 \t 0.874883 \t 0.709913\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_estimator.predict(X)\n",
    "\n",
    "print(f\"f1 \\t\\t precision \\t recall\")\n",
    "print(f\"{f1_score(y,  y_pred):.6f} \\t {precision_score(y,  y_pred):.6f} \\t {recall_score(y,  y_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions sur le jeu sans label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(31620, 5)\n"
     ]
    }
   ],
   "source": [
    "df_no_labels = pd.read_csv('./assets/conversion_data_test.csv')\n",
    "print(type(df_no_labels))\n",
    "print(df_no_labels.shape)\n",
    "\n",
    "# X_no_labels = add_weight_col(df_no_labels) \n",
    "\n",
    "\n",
    "X_no_labels = df_no_labels\n",
    "X_no_labels = preprocessor.transform(X_no_labels)\n",
    "\n",
    "# clf = XGBClassifier()\n",
    "# clf.set_params(**best_params)\n",
    "y_no_labels = best_estimator.predict(X_no_labels) \n",
    "\n",
    "# print(type(X_no_labels))\n",
    "# print(X_no_labels.shape)\n",
    "\n",
    "# print(type(X_no_labels))\n",
    "# print(X_no_labels.shape)\n",
    "# print(X_no_labels[0:5,:].round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des prédictions\n",
    "\n",
    "data = {\n",
    "  'converted': y_no_labels\n",
    "}\n",
    "\n",
    "Y_predictions = pd.DataFrame(columns=['converted'], data=data)\n",
    "\n",
    "trailer         = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_file = \"./assets/\" + header + author + \"-\" + trailer + \".csv\"\n",
    "Y_predictions.to_csv(out_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si je souhaite insérer une fonction (ou une classe ) dans un pipeline de traitement, comment dois je l'écrire ?\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# # Définir la fonction à insérer dans le pipeline\n",
    "# def custom_function(X):\n",
    "#     # Faire quelque chose avec X\n",
    "#     # Par exemple, ajouter 1 à chaque valeur\n",
    "#     return X + 1\n",
    "\n",
    "# # Créer un pipeline avec la fonction encapsulée dans FunctionTransformer\n",
    "# pipeline = Pipeline([\n",
    "#     ('custom_transformer', FunctionTransformer(custom_function))\n",
    "# ])\n",
    "\n",
    "# # Utiliser le pipeline sur les données\n",
    "# X_transformed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, parameter1, parameter2):\n",
    "#         self.parameter1 = parameter1\n",
    "#         self.parameter2 = parameter2\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Réaliser des opérations d'initialisation ou d'apprentissage ici\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         # Réaliser la transformation des données ici\n",
    "#         transformed_X = X + self.parameter1\n",
    "#         transformed_X *= self.parameter2\n",
    "#         return transformed_X\n",
    "\n",
    "# # Créer un pipeline avec la classe personnalisée\n",
    "# pipeline = Pipeline([\n",
    "#     ('custom_transformer', CustomTransformer(parameter1=1, parameter2=2))\n",
    "# ])\n",
    "\n",
    "# # Utiliser le pipeline sur les données\n",
    "# X_transformed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOIR AUSSI\n",
    "\n",
    "# # Création du pipeline avec des étapes de prétraitement et un estimateur final\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessing', CustomTransformer()),  # Transformer personnalisé\n",
    "#     ('estimator', RandomForestClassifier())  # Estimateur final\n",
    "# ])\n",
    "\n",
    "# # Apprentissage sur l'ensemble d'entraînement\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Prédiction sur l'ensemble de test\n",
    "# y_pred = pipeline.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
