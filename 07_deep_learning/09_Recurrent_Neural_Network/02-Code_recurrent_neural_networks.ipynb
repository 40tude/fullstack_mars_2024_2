{"cells":[{"cell_type":"markdown","metadata":{"id":"Va9BZ1QuNMKQ"},"source":["# Code recurrent neural networks\n","\n","This demo will walk you through how to build recurrent neural networks to solve problems with text data (these methods may also be used for any sequential data like time series, sound etc...)\n","\n","## What will you learn in this course? üßêüßê\n","\n","This course will focus on the technical approach to building recurrent neural networks and details on how to code the three new layers we have studied!\n","Here is the outline:\n","\n","* Recurrent layers\n","  * SimpleRNN\n","  * GRU\n","  * LSTM\n","* Build a recurrent neural network"]},{"cell_type":"markdown","metadata":{"id":"nOyBw-XTN-yc"},"source":["## Recurrent layers\n","\n","In this section we will focus strictly on studying the code around the three new layers we just learned about: simpleRNN, GRU, and LSTM."]},{"cell_type":"markdown","metadata":{"id":"FJVeimLTOJCN"},"source":["# SimpleRNN --------------------------------------------\n","\n","\n","The most simple recurrent layer corresponds to `tf.keras.layers.SimpleRNN`, you may find the documentation here [simpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JieP1O9a6g0w"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import SimpleRNN\n","\n","\n","# units            = number of neurons in the layer\n","# return_sequences = whether the layer should output the full sequence of outputs computed while processing the input sequence or just the last output\n","# return_state     = to return the hidden state in a separate object\n","\n","\n","\n","# On definit un SimpleRNN avec 16 neurones\n","# Chaque neurone va √™tre connect√© aux 4 colonnes de la matrice WordEmbedding qu'on va lui pr√©senter\n","# Il va faire Phi(Sigma .... + h w + b) sur chacun des 4 valeurs de la ligne\n","# On aura donc une ligne de 16 valeurs\n","# On aura donc 10 lignes et 16 colonnes\n","srnn = SimpleRNN(units=16, return_sequences=False, return_state=False)\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"rZS3JrS9ZcyK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715680659343,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"WxmGGc1_AMgb","outputId":"6e4daff2-1d6e-4eb0-eced-0f4e5bf3dcf3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 10, 4), dtype=float32, numpy=\n","array([[[ 1.7618275e-03, -1.1001991e+00,  1.7366928e-01, -8.9706898e-01],\n","        [-1.1664591e+00,  1.3106228e+00, -1.1294969e+00, -4.6602622e-01],\n","        [-1.8397214e-01, -2.7872634e-01, -1.0606695e+00, -1.8988892e-01],\n","        [ 4.8748791e-02,  7.6717454e-01,  1.1084483e+00, -1.1009997e+00],\n","        [ 5.3341228e-01,  7.9885697e-01,  8.0976820e-01,  5.2871376e-02],\n","        [-1.1784338e+00,  8.5470957e-01,  1.3981770e+00, -2.0381522e+00],\n","        [-7.7480239e-01, -1.0039926e+00, -2.0655198e+00, -2.2370915e+00],\n","        [-5.7550144e-01, -1.5513005e+00,  7.7024323e-01, -2.6107851e-01],\n","        [-5.7658273e-01,  2.1991391e+00,  9.8297253e-02, -1.8843707e+00],\n","        [ 1.6924981e+00, -2.6202568e-01,  2.2290002e-01, -6.7218447e-01]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":2}],"source":["# Ici on \"simule\" la sortie d'un Embeding Output\n","# 4 colonnes et 10 lignes\n","# lignes   = nombre de mots seq_len\n","# colonnes = une puissance de 2\n","\n","# Let's create an example input for this layer and see how it works\n","batch_size  = 1\n","seq_len     = 10\n","channels    = 4\n","\n","input = tf.random.normal(shape=(batch_size, seq_len, channels))\n","input"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":349,"status":"ok","timestamp":1715680659690,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"zrbMMSccBjY4","outputId":"2885903e-976c-4dad-d6d0-c6aa496e08d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n","array([[ 0.48304415, -0.94882745, -0.47727525, -0.04252353, -0.60917974,\n","        -0.84126204,  0.7383893 , -0.45081544, -0.5453766 , -0.5817851 ,\n","         0.16214931,  0.75372493,  0.42567596,  0.44439647, -0.08026829,\n","         0.17734928]], dtype=float32)>"]},"metadata":{},"execution_count":3}],"source":["# Voir qu'on a bien un vecteur 16 en sortie\n","# En fait √† la cr√©ation de RNN on a indiqu√©\n","# return_sequences=False        => on retourne la derni√®re ligne (qui tient compte des 9 lignes pr√©c√©dente)\n","# return_state=False            => on ne retourne pas lles hidden state\n","\n","# En entree on a un tenseur 1 x 10 x 4\n","# En sortie on a un tenseur 1 x 16\n","\n","\n","# now let's apply the simpleRNN layer and see what comes out\n","srnn(input)\n","\n","# the ouput is a batch of one observation with 16 representation channels which\n","# corresponds to the number of units in the layer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715680659690,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"kXJGfMGsB8kn","outputId":"ea38d8e2-d1b8-42d6-e0d0-d2bab1e592d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 10, 16), dtype=float32, numpy=\n","array([[[-0.16674514, -0.5428698 , -0.2414192 ,  0.65495515,\n","          0.16298962, -0.561073  ,  0.35638264,  0.08658373,\n","          0.13164768, -0.3832713 ,  0.3170101 , -0.05791348,\n","          0.01584603, -0.52405834,  0.57561624, -0.22586507],\n","        [ 0.02148461,  0.8013764 , -0.7834751 , -0.5617418 ,\n","          0.77003765,  0.9167274 , -0.3608897 ,  0.0302986 ,\n","         -0.6911191 , -0.74479264, -0.27278763, -0.22496752,\n","          0.5018158 , -0.210946  , -0.10704214,  0.55836844],\n","        [ 0.46089667,  0.40629053,  0.2596361 , -0.89198124,\n","          0.45567492,  0.49589124, -0.89426714, -0.33622608,\n","          0.44279096,  0.41038287, -0.48679465,  0.10893589,\n","         -0.06289853, -0.53423417,  0.8834528 ,  0.39269677],\n","        [-0.6661824 ,  0.06607206,  0.17518276,  0.05739673,\n","         -0.8824435 ,  0.47650233,  0.48217472,  0.7331726 ,\n","         -0.09654682,  0.24278483,  0.9289079 , -0.8789382 ,\n","          0.8871039 , -0.772139  , -0.53453547,  0.61183006],\n","        [ 0.27160263, -0.21551953, -0.01029276, -0.40977484,\n","          0.3165668 , -0.5625984 ,  0.7792362 ,  0.2882915 ,\n","          0.5087395 ,  0.06997339,  0.9043935 ,  0.38223973,\n","         -0.5928931 ,  0.36880407, -0.21209106,  0.7883152 ],\n","        [-0.5290874 ,  0.6512202 ,  0.07223329,  0.6592092 ,\n","         -0.46780798, -0.77489614,  0.85298216,  0.46354792,\n","         -0.57797223, -0.8884884 ,  0.9379683 , -0.88663995,\n","          0.6985342 , -0.52608544, -0.52844614, -0.20653042],\n","        [ 0.6482802 ,  0.14080882, -0.99113905, -0.21446106,\n","          0.9814402 ,  0.05466115, -0.5981487 , -0.6121225 ,\n","         -0.6919221 , -0.907883  , -0.46155983,  0.317123  ,\n","         -0.71512663, -0.70114076,  0.97012204,  0.7588122 ],\n","        [-0.6109123 , -0.02809589,  0.87341195,  0.53498864,\n","          0.45679626, -0.5650965 , -0.7045133 ,  0.63589746,\n","          0.49850726, -0.19343433, -0.10560562,  0.087542  ,\n","          0.6167085 , -0.92665213,  0.18120165, -0.46477038],\n","        [-0.36288515,  0.6836021 , -0.8157713 ,  0.6341455 ,\n","         -0.6101334 ,  0.9038193 ,  0.602923  ,  0.7914677 ,\n","         -0.59537095, -0.8311015 ,  0.87420475, -0.941023  ,\n","          0.8157037 , -0.31743556, -0.2777177 ,  0.987356  ],\n","        [ 0.3759922 , -0.6831507 , -0.52433026,  0.19747446,\n","          0.91618437, -0.94079614,  0.10008825,  0.01573506,\n","          0.89753675,  0.31648228,  0.8655049 , -0.07934115,\n","         -0.30822384, -0.613234  ,  0.9238126 ,  0.4905708 ]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":4}],"source":["# On demande\n","# return_sequences=True       => On retourne toute la s√©quence de calcul => 10 lignes x 16 colonne\n","# return_state=False          => On retorune pas les hidden states\n","\n","# let's change things up by returning the whole output sequence\n","srnn = SimpleRNN(units=16, return_sequences=True, return_state=False)\n","\n","srnn(input)\n","# now the layer preserves the sequential structure of the input, instead of\n","# returning a 2D tensor, now outputs a 3D tensor of shape (batch_size, seq_len, units)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1715680660045,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"08XCE51rG-AD","outputId":"1f9f7101-d990-44a2-aa14-d860d22dfe39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n"," array([[-0.18483947,  0.9103874 ,  0.75096023, -0.16370803,  0.42190883,\n","          0.48930535,  0.35711202,  0.45196533,  0.8122    ,  0.48135474,\n","          0.05689316,  0.35409465, -0.3624563 ,  0.8596767 , -0.5756049 ,\n","          0.7684309 ]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n"," array([[-0.18483947,  0.9103874 ,  0.75096023, -0.16370803,  0.42190883,\n","          0.48930535,  0.35711202,  0.45196533,  0.8122    ,  0.48135474,\n","          0.05689316,  0.35409465, -0.3624563 ,  0.8596767 , -0.5756049 ,\n","          0.7684309 ]], dtype=float32)>]"]},"metadata":{},"execution_count":5}],"source":["# On demande\n","# return_sequences=False            =>  on retroune que la derni√®re s√©quence 1 ligne x 16 colonnes\n","# return_state=True                 =>  on retourne les hidden states. Pas d'interr√™t ici car RNN. retourne 2 fois le m√™me ligne\n","\n","\n","srnn = SimpleRNN(units=16, return_sequences=False, return_state=True)\n","\n","srnn(input)\n","# now the layer returns two objects, the output and the hidden state, well in\n","# simpleRNN they carry the same values as you can see"]},{"cell_type":"markdown","metadata":{"id":"R1Dxj6TXOKsK"},"source":["# GRU --------------------------------------------\n","\n","Now let's see how we can code GRU layers, you can read the documentation here: [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1715680660045,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"JQ4gAQLQHxYW","outputId":"1a0e040d-fd31-4a74-9bbc-1ccad21d2b6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n","array([[ 0.04443589,  0.04310554, -0.18226852,  0.2776561 ,  0.0530444 ,\n","         0.07345533, -0.3263973 , -0.02632806, -0.21374685, -0.09900159,\n","         0.04404004,  0.42702842,  0.08360571, -0.02835929, -0.1385899 ,\n","         0.22601241]], dtype=float32)>"]},"metadata":{},"execution_count":6}],"source":["# return_sequences = False      => derni√®re ligne 1 x 16\n","# return_state     = False      => rien\n","\n","from tensorflow.keras.layers import GRU\n","\n","gru = GRU(units=16, return_sequences=False, return_state=False)\n","\n","gru(input)\n","\n","# it works mainly in the same way as the SimpleRNN layer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":434,"status":"ok","timestamp":1715680660476,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"oEdJ_-ZlIibX","outputId":"4ae2d381-d9f5-4444-875f-f840250c1f46"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 10, 16), dtype=float32, numpy=\n","array([[[-1.33378953e-01, -4.02516685e-02, -1.39565051e-01,\n","          1.87363252e-01, -7.61715602e-03,  1.97818577e-01,\n","         -8.18353668e-02, -9.57111456e-03,  5.55594936e-02,\n","         -6.21235929e-02, -2.00618878e-01,  1.04959853e-01,\n","          8.56895447e-02,  4.80387546e-03,  7.16334805e-02,\n","         -7.37120807e-02],\n","        [-4.47026417e-02,  4.92143035e-02,  1.18637614e-01,\n","         -1.25896230e-01,  3.55513602e-01,  1.80481613e-01,\n","          2.62508035e-01, -2.30472192e-01,  1.26995906e-01,\n","         -2.00958058e-01, -1.37123019e-02, -1.86547294e-01,\n","         -1.19403601e-02,  5.85986525e-02,  1.20771416e-01,\n","         -3.67411792e-01],\n","        [-1.47925122e-02,  3.97363082e-02,  5.97124770e-02,\n","         -8.86607692e-02,  2.40378648e-01,  3.07722807e-01,\n","          2.07008332e-01, -2.63900757e-01,  5.23323640e-02,\n","         -4.59286347e-02,  1.06635876e-01,  2.35873014e-02,\n","         -7.74969459e-02,  4.22633663e-02,  2.28877574e-01,\n","         -3.48241776e-01],\n","        [ 1.14365071e-01,  8.61017630e-02,  2.62826562e-01,\n","          6.57920912e-03,  1.80205211e-01,  7.57548362e-02,\n","          7.03079179e-02,  1.19240813e-01,  2.07135931e-01,\n","         -2.92010546e-01, -2.65307456e-01, -1.47266641e-01,\n","          1.98032945e-01, -8.08785185e-02, -2.34176815e-01,\n","         -1.80047706e-01],\n","        [ 1.89935744e-01,  3.29063758e-02,  2.80730367e-01,\n","          2.66575487e-04, -3.42555195e-02, -1.74770743e-01,\n","         -5.11293001e-02,  2.53118515e-01,  1.92206010e-01,\n","         -2.28472114e-01, -1.49492607e-01, -1.69017226e-01,\n","          2.66455710e-01, -1.29403204e-01, -3.71909618e-01,\n","          8.32736567e-02],\n","        [-6.08978197e-02,  1.31155729e-01,  3.94680381e-01,\n","          9.34449807e-02,  3.93504143e-01, -1.89596415e-01,\n","          5.28860651e-02,  2.69490272e-01,  3.92556012e-01,\n","         -5.18690228e-01, -5.68986416e-01, -2.98671216e-01,\n","          5.14876127e-01, -2.10693941e-01, -4.25213009e-01,\n","         -2.34992862e-01],\n","        [-1.91045821e-01, -7.99984485e-03,  1.41994059e-01,\n","          7.57453293e-02,  5.00145197e-01,  1.76319569e-01,\n","          2.72608668e-01, -2.62415111e-01,  3.46898884e-01,\n","         -3.97526562e-01, -2.98598528e-01,  1.69912100e-01,\n","          2.39049435e-01,  3.28940153e-03, -9.79349166e-02,\n","         -4.12553489e-01],\n","        [-2.99052566e-01,  1.14440084e-01, -9.70105529e-02,\n","          3.29758495e-01,  1.90143675e-01,  1.60317779e-01,\n","         -2.78011560e-02, -1.86100155e-01,  8.11427161e-02,\n","         -1.66363165e-01, -3.77696097e-01,  2.18902767e-01,\n","          1.49154842e-01, -1.41533196e-01,  1.63896233e-01,\n","         -1.95568904e-01],\n","        [-1.37422383e-02,  9.96520892e-02,  4.44188386e-01,\n","          7.63872266e-03,  5.34120679e-01,  8.57146308e-02,\n","          2.67540216e-01,  2.71716118e-02,  3.54833663e-01,\n","         -3.76109302e-01, -4.03937697e-01, -2.44895816e-01,\n","          3.89279932e-01, -1.07048657e-02, -2.61670083e-01,\n","         -4.43654090e-01],\n","        [ 1.26404643e-01, -1.87589690e-01,  1.15493804e-01,\n","          1.06880330e-01, -1.65549487e-01,  7.88777769e-02,\n","         -1.20087817e-01,  1.66526482e-01,  3.19704980e-01,\n","         -1.59369409e-01, -1.88929096e-01, -2.50695944e-02,\n","          4.25385237e-01, -3.10861543e-02, -3.74772847e-01,\n","         -2.37865210e-01]]], dtype=float32)>"]},"metadata":{},"execution_count":7}],"source":["# return_sequences = True      => 10 x 16\n","# return_state     = False     => rien\n","\n","\n","gru = GRU(units=16, return_sequences=True, return_state=False)\n","\n","gru(input)\n","\n","# you can still use return_sequences in order to preserve the sequential\n","# nature of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715680660477,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"fyY8OkghI5Qb","outputId":"e999eb06-4ab1-4e71-c75a-e1608a819092"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(1, 10, 16), dtype=float32, numpy=\n"," array([[[-0.00455865, -0.03974918,  0.20154165, -0.09431591,\n","          -0.00615946, -0.23103425, -0.02805037, -0.04575203,\n","           0.0145016 , -0.05197488,  0.19050054,  0.22756319,\n","           0.01435653,  0.25703236, -0.01960542,  0.05950272],\n","         [ 0.23313503, -0.13895512, -0.05731003,  0.32369846,\n","           0.09982147, -0.03538036, -0.1609427 ,  0.02978718,\n","          -0.02560422, -0.04678731,  0.239238  ,  0.01927432,\n","          -0.09917975,  0.26365498, -0.01385885,  0.04363044],\n","         [ 0.3226183 , -0.14882174,  0.00483473,  0.29418123,\n","          -0.02612038,  0.00973784, -0.02595407, -0.05855656,\n","          -0.01852   , -0.08050199,  0.22272407,  0.19415647,\n","          -0.16394046,  0.29052866, -0.09591301,  0.0501655 ],\n","         [ 0.11655328, -0.12382317, -0.0524689 ,  0.1832883 ,\n","           0.16511336, -0.12490445, -0.2552358 ,  0.20736434,\n","           0.04958502,  0.05115388,  0.10890801, -0.10011427,\n","           0.03852873,  0.0030352 ,  0.21915197,  0.30306676],\n","         [-0.04558048,  0.02386626, -0.1552151 ,  0.04861966,\n","           0.15433678, -0.04016983, -0.10198114,  0.33090287,\n","           0.08613478,  0.10214466, -0.14306101, -0.20566934,\n","           0.04521864, -0.2521696 ,  0.30075353,  0.24322869],\n","         [-0.1938822 , -0.05511536, -0.28219226,  0.10766068,\n","           0.4123189 , -0.23398097, -0.4800528 ,  0.39974648,\n","           0.05790304,  0.17804676,  0.11357268, -0.38447243,\n","           0.21237575, -0.09091386,  0.37815654,  0.29659194],\n","         [ 0.35417643, -0.2263854 , -0.03879256,  0.3264649 ,\n","          -0.08417539, -0.4069828 , -0.30858475,  0.30594146,\n","           0.03252832, -0.1832229 ,  0.49613166,  0.389342  ,\n","          -0.15339965,  0.5772613 ,  0.05521557,  0.33587423],\n","         [ 0.1499382 , -0.13011052,  0.26628268, -0.06297921,\n","           0.14129195, -0.53658456, -0.25398663,  0.07422861,\n","          -0.02399297, -0.0036907 ,  0.4527158 ,  0.29218304,\n","           0.06326219,  0.5414708 , -0.16445784,  0.00716452],\n","         [ 0.12648779, -0.22380337, -0.00088654,  0.23358995,\n","           0.3109845 , -0.44985834, -0.46599945,  0.37530434,\n","           0.01880628,  0.01857742,  0.385362  , -0.09782223,\n","          -0.00564657,  0.21161836,  0.21578437,  0.42390803],\n","         [ 0.05858099, -0.01694041,  0.08241806,  0.05450387,\n","           0.08264264, -0.2956733 ,  0.11381634,  0.40933776,\n","           0.16540517, -0.11312476,  0.08506059,  0.20231608,\n","          -0.13502544, -0.18725929,  0.3581356 ,  0.47658378]]],\n","       dtype=float32)>,\n"," <tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n"," array([[ 0.05858099, -0.01694041,  0.08241806,  0.05450387,  0.08264264,\n","         -0.2956733 ,  0.11381634,  0.40933776,  0.16540517, -0.11312476,\n","          0.08506059,  0.20231608, -0.13502544, -0.18725929,  0.3581356 ,\n","          0.47658378]], dtype=float32)>]"]},"metadata":{},"execution_count":8}],"source":["# return_sequences = True     => 10 x 16\n","# return_state     = True     => hidden state 1 x 16\n","\n","\n","# voir return_sequences et return_states **************************************************\n","gru = GRU(units=16, return_sequences=True, return_state=True)\n","\n","gru(input)\n","\n","# the state is always equal to the values returned after processing the whole\n","# sequence"]},{"cell_type":"markdown","metadata":{"id":"GYf2DkyJOL2j"},"source":["### LSTM --------------------------------------------\n","\n","Last but not least let's see how to code an LSTM neuron, check the documentation:\n","[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1715680660816,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"iigMjAFhMwqn","outputId":"5b46c6da-bc72-4288-f880-b081bcb1d7a6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n","array([[ 0.0437494 ,  0.25056836, -0.09685995, -0.00147228,  0.01740794,\n","        -0.06458052,  0.0883551 ,  0.02214705, -0.15541825,  0.048528  ,\n","        -0.14544794,  0.02113206, -0.05433153,  0.17134427,  0.36067814,\n","        -0.07412378]], dtype=float32)>"]},"metadata":{},"execution_count":9}],"source":["# return_sequences=False, return_state=False => retourne la derni√®re \"ligne\" => 1 x 16\n","\n","from tensorflow.keras.layers import LSTM\n","\n","lstm = LSTM(units=16, return_sequences=False, return_state=False)\n","\n","lstm(input)\n","\n","# it works mainly like GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715680660816,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"vk9oezEKNLrL","outputId":"87a27078-aace-43c7-bb4a-6cd0d0b596e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(1, 10, 16), dtype=float32, numpy=\n"," array([[[-0.05265173, -0.00422145, -0.06425991,  0.01683652,\n","          -0.03372828, -0.00222418,  0.04200807, -0.01008242,\n","           0.00220767, -0.01172854, -0.0423681 , -0.06866197,\n","          -0.00223496, -0.12223417, -0.1182522 ,  0.00558752],\n","         [ 0.03026772,  0.01093519,  0.07037749, -0.04660639,\n","           0.08601566,  0.06559689,  0.01094318, -0.12858419,\n","           0.02175834, -0.00668694,  0.031776  , -0.02510739,\n","          -0.09749302, -0.02072402,  0.02440318, -0.12334388],\n","         [-0.04334528,  0.0566337 ,  0.13668896, -0.07513575,\n","           0.04251193,  0.11802781,  0.07696208, -0.1397194 ,\n","          -0.01389325,  0.03883779,  0.02852535, -0.01484615,\n","          -0.11613293, -0.00967928,  0.04315626, -0.07691305],\n","         [ 0.02412329, -0.04597352,  0.08547155, -0.05710659,\n","           0.11342768, -0.03328148, -0.06315617, -0.08509945,\n","           0.05219471, -0.04882702,  0.08838362, -0.05723761,\n","           0.00662832, -0.04247846, -0.0234362 , -0.12670685],\n","         [ 0.06763849, -0.08517725,  0.07434099, -0.02083447,\n","           0.11085884, -0.17425458, -0.12422837, -0.0147668 ,\n","           0.06254639, -0.06708186,  0.12503088, -0.01878961,\n","           0.11605071, -0.00908137, -0.00567629, -0.10453653],\n","         [ 0.17556164, -0.12070696,  0.05015628, -0.07955939,\n","           0.22162916, -0.20062682, -0.2617693 , -0.03132393,\n","           0.13035057, -0.22271684,  0.11725086, -0.1088323 ,\n","           0.20373477, -0.11501332, -0.10467925, -0.23483783],\n","         [-0.00603141, -0.18292573,  0.18983862, -0.14970635,\n","           0.08919432, -0.02706794, -0.16204433, -0.12422039,\n","           0.01771653, -0.04616191,  0.0428992 , -0.11820305,\n","          -0.00377592, -0.12038416, -0.25011086, -0.29853463],\n","         [ 0.00339844, -0.09502444,  0.06050829, -0.10869415,\n","          -0.03464058,  0.00252453, -0.01735163, -0.06744802,\n","           0.06295279, -0.12050787, -0.07652476, -0.29193673,\n","           0.00754487, -0.3420612 , -0.20964   , -0.20313165],\n","         [ 0.1047235 , -0.1236058 ,  0.10921137, -0.10488392,\n","           0.16592462, -0.14095521, -0.16417822, -0.12322275,\n","           0.10336885, -0.1651133 ,  0.06632622, -0.10957179,\n","           0.09057039, -0.11274526, -0.19248787, -0.2034246 ],\n","         [-0.04788401, -0.20767663,  0.13537604, -0.05436416,\n","           0.11009366, -0.24230908, -0.150934  , -0.02190169,\n","           0.03612629, -0.08054426,  0.09177466, -0.09941558,\n","           0.1489354 , -0.18424681, -0.19306593, -0.12043492]]],\n","       dtype=float32)>,\n"," <tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n"," array([[-0.04788401, -0.20767663,  0.13537604, -0.05436416,  0.11009366,\n","         -0.24230908, -0.150934  , -0.02190169,  0.03612629, -0.08054426,\n","          0.09177466, -0.09941558,  0.1489354 , -0.18424681, -0.19306593,\n","         -0.12043492]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n"," array([[-0.08294739, -0.43542293,  0.3003195 , -0.10067258,  0.2237826 ,\n","         -0.47061026, -0.39903188, -0.05553582,  0.09661134, -0.20171623,\n","          0.19810736, -0.23064601,  0.36000428, -0.31980434, -0.38421553,\n","         -0.32362545]], dtype=float32)>]"]},"metadata":{},"execution_count":10}],"source":["# cell state est celui qui est diff√©rent\n","# hidden state est identique √† la derni√®re ligne\n","\n","\n","# return_sequences=True       => 10 x 16\n","# return_state=True           => 1 x 16 + 1 x 16\n","\n","lstm = LSTM(units=16, return_sequences=True, return_state=True)\n","\n","lstm(input)\n","\n","# When using return_state, the layer returns\n","# 1 the output (sequence or not depending on return_sequences)\n","# 2 the hidden state which is equal to the final output\n","# 3 the cell state"]},{"cell_type":"markdown","metadata":{"id":"2U2HpYlROSc2"},"source":["Now that you know how to code the three different recurrent layers, let's look to build a recurrent neural network on text data."]},{"cell_type":"markdown","metadata":{"id":"UwzRBiroONUH"},"source":["## Build a recurrent network"]},{"cell_type":"markdown","metadata":{"id":"e6nwJV3xPkAx"},"source":["Let's show you an example on some toy dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqtF4ChPM23g"},"outputs":[],"source":["# on passe √† une archi compl√®te\n","# debut = copier coller d'hier\n","# tout en tensor flow\n","\n","import io\n","import os\n","import re\n","import shutil\n","import string\n","import tensorflow as tf\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"]},{"cell_type":"markdown","metadata":{"id":"ykIM87xnwg17"},"source":["We'll use the same dataset we used for the embedding and word2vec demos which is the movie critique dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":42187,"status":"ok","timestamp":1715680702995,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"J1O1JVh4VqdW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f140685-88e2-4c10-ac7a-30d0000cda57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84125825/84125825 [==============================] - 6s 0us/step\n"]}],"source":["url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","\n","dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n","                                  untar=True, cache_dir='.',\n","                                  cache_subdir='')\n","\n","# after dowloading the data we remove the unlabeled examples stored in the\n","# unsup folder\n","remove_dir = os.path.join(\"/content/aclImdb/train\", 'unsup')\n","shutil.rmtree(remove_dir)"]},{"cell_type":"markdown","metadata":{"id":"4Ebv4o1gx2pF"},"source":["Now let's proceed to load the data into a batch generator"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2963,"status":"ok","timestamp":1715680705944,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"IB95a31VV0G4","outputId":"520e9703-500a-4098-cb5e-932fc48ad62f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 2 classes.\n","Using 20000 files for training.\n","Found 25000 files belonging to 2 classes.\n","Using 5000 files for validation.\n"]}],"source":["batch_size  = 128\n","seed        = 123                     # seed is mandatory here to prevent overlap between train and validation\n","\n","train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train',                  # path to the folder containing the text files\n","    batch_size=batch_size,            # the size of a batch of data\n","    validation_split=0.2,             # The proportion of data in the validation set\n","    subset='training',                # Forms the train set\n","    seed=seed)                        # similar to random_state\n","\n","val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train',\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='validation',              # forms the validation set\n","    seed=seed)"]},{"cell_type":"markdown","metadata":{"id":"g21cpyyEx9Qz"},"source":["Let's take a look at a batch of data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456,"status":"ok","timestamp":1715680706379,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"_iNWrNx3V3bo","outputId":"d87f24f1-3c15-484f-f9d7-59885cc6f3ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 b\"I have watched this movie well over 100-200 times, and I love it each and every time I watched it. Yes, it can be very corny but it is also very funny and enjoyable. The camp shown in the movie is a real camp that I actually attended for 7 years and is portrayed as camp really is, a great place to spend the summer. Everyone who has ever gone to camp, wanted to go to camp, or has sent a child to camp should see this movie because it'll bring back wonderful memories for you and for your kids.\"\n","1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n","1 b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n","1 b\"I first saw this movie here in the U.K. in December 1989 when Central TV broadcast it. I still have the video tape, although worn out (over the years many friends and family members have borrowed it and have also been chilled by it!). <br /><br />Anyway, I remember coming home that night, grabbing a Christmas tipple, switching the lights out and watching what was advertised as a 'Christmas Ghost Story'. Even now I remember certain scenes that still send the hairs on my neck standing on-end... <br /><br />I have seen some comments on the movie which say it's not this and not that...I think those people get scared by Friday 13th and the like, stalk and slash rammel, which are laughable. This is a 'traditional' ghost story; there is no big budget action or special effects...no swearing, no blood, no gratuitous sex scenes, no chainsaws or guns etc...So how refreshing!!!! It's atmospheric. IF you like chilling horror, well written, well acted and with a genuinely scary atmosphere, this is the movie for you. I like the original horrors; only last night I saw the original Haunting and that is a superb movie. Very atmospheric again - and so is The Woman In Black. The end of the movie differs to the book, but still very good. I recommend it. Try it...you *will* like it if you like traditional ghost stories...SO...turn off the lights, turn up the fire, lock the doors, grab a drink...and enjoy... :)\"\n","0 b'If you\\'re looking to be either offended or amused or both, you\\'ll probably have to look elsewhere. LMOTP really isn\\'t even very thought provoking beyond rehashing the usual silly clich\\xc3\\xa9s. At the end of the second episode I felt a little embarrassed that I actually sat through the contrived mess.<br /><br />Beyond the thinly veiled gimmicky premise thats attracted all the initial attention to it in the first place it\\'s just another lame, innocuous and anti-septic attempt at commentary and entertainment that the CBC typically excels at producing. And once the \"ZOMG MUSLIMS IN RURAL CANADA ROFLMAO!!\" hype wears out its welcome, the show is likely to follow into the ether of cancellation because it\\'s so shallow when judged on its merits alone.<br /><br />Unless you\\'re obsessed with Muslim culture in the west and/or are easily amused by the most minute idiosyncrasies on the subject I really don\\'t see how LMOTP is enjoyable beyond satisfying the curiosity that stemmed from the hype. Other shows have better addressed the issue of cultural/ethnic dichotomy in western multi-ethnic societies. LMOTP will never rank among them in entertainment or insight.'\n"]}],"source":["for text_batch, label_batch in train_ds.take(1):\n","  for i in range(5):\n","    print(label_batch[i].numpy(), text_batch.numpy()[i])"]},{"cell_type":"markdown","metadata":{"id":"QsJIenKRyAXt"},"source":["There's some preprocessing to be done, it's possible to do it with spacy by loading all the texts in memory and removing stop words and lemmatize tokens, but the more memory friendly way to do this is to create a preprocessing layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzKT7y43V8sC"},"outputs":[],"source":["# Create a custom standardization function to strip HTML break tags '<br />'.\n","\n","def custom_standardization(input_data):\n","  # transform all characters to lowercase\n","  lowercase = tf.strings.lower(input_data)\n","\n","  # remove all <br and /> strings\n","  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n","\n","  # replace punctuation with empty string\n","  # [%s] % re.escape(string.punctuation) is a formatting syntax borrowed to see\n","  # [] creates a group, and the %s gets replaced by the content of\n","  # re.escape(string.punctuation) (the escaped punctuation characters)\n","  return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n","\n","\n","# Vocabulary size and number of words in a sequence.\n","vocab_size      = 10_000\n","sequence_length =    100\n","\n","# Use the text vectorization layer to normalize, split, and map strings to\n","# integers. Note that the layer uses the custom standardization defined above.\n","# Set maximum_sequence length as all samples are not of the same length.\n","vectorize_layer = TextVectorization(\n","    standardize             = custom_standardization,   # string tensor input -> string tensor output\n","    max_tokens              = vocab_size,               # int, keep only the vocab_size most common tokens\n","    output_mode             = 'int',                    # sets the type of encoding\n","    output_sequence_length  = sequence_length)          # truncates or pads sequences to a certain length\n","\n","# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n","text_ds = train_ds.map(lambda x, y: x) # this is building a text only tf dataset\n","vectorize_layer.adapt(text_ds) # lists the vocab and the most common words"]},{"cell_type":"markdown","metadata":{"id":"MQPVhsPk28GI"},"source":["Now let's define a model including some recurrent neurons. Note that if you wish to stack recurrent layers you have to preserve the sequential nature of the data with `return_sequence=True`, the last recurrent may use `return_sequence=False` this will flatten the data so you can use dense layers afterwards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHbDdC7UWAW2"},"outputs":[],"source":["# vocab_size = 10_000 => doit tenir compte de 9999 + 1 de padding\n","\n","# En calcul, les couche RNN sont gourmandes\n","# C'est comme si on avait 100 couche dense... En gros\n","\n","# parametre pour envoyer derner element de la seque\n","# return_sequence (defaut = True) si on met False il renvoie le\n","# Y a aussi un return state => renvoie hidden state et cell state (depend de RNN, GRU, LSTM...)\n","\n","# return sequence = True car on va enchainer sur un 2nd  RNN\n","# La derni√®re couche de SimpleRNN il faut mettre return_sequences=False\n","# Pas de regle sur les 64 et 32 des units des SimpleRNN\n","#     64 analyse pr√©cise\n","#     32 analyse de synthetise\n","\n","embedding_dim=32                                              # the dimensionality of the representation space\n","model = Sequential([\n","  vectorize_layer,                                            # This layers encodes the string as sequences of int\n","  Embedding(vocab_size, embedding_dim, name=\"embedding\"),     # the embedding layer the input dim needs to be equal to the size of the vocabulary + 1 (because of the zero padding)\n","  SimpleRNN(units=64, return_sequences=True),                 # maintains the sequential nature\n","  SimpleRNN(units=32, return_sequences=False),                # returns the last output\n","  Dense(16, activation='relu'),                               # a dense layer\n","  Dense(1, activation=\"sigmoid\")                              # the prediction layer\n","])"]},{"cell_type":"markdown","metadata":{"id":"TFZx3-gg4qpI"},"source":["We need to compile the model so it can train on the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDhWp7qg3LVX"},"outputs":[],"source":["model.compile(optimizer = 'adam',\n","              loss      = tf.keras.losses.BinaryCrossentropy(),\n","              metrics   = ['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"LpQT2l3E4wEu"},"source":["We can now train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162420,"status":"ok","timestamp":1715680871639,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"lLyvaid93Nto","outputId":"7dd94c95-9294-4c9d-a90c-1c7d12e6654c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","157/157 [==============================] - 19s 100ms/step - loss: 0.6969 - accuracy: 0.5153 - val_loss: 0.6952 - val_accuracy: 0.4886\n","Epoch 2/10\n","157/157 [==============================] - 16s 99ms/step - loss: 0.6933 - accuracy: 0.5025 - val_loss: 0.6928 - val_accuracy: 0.5128\n","Epoch 3/10\n","157/157 [==============================] - 16s 99ms/step - loss: 0.6919 - accuracy: 0.5221 - val_loss: 0.6926 - val_accuracy: 0.5036\n","Epoch 4/10\n","157/157 [==============================] - 15s 97ms/step - loss: 0.6818 - accuracy: 0.5664 - val_loss: 0.7161 - val_accuracy: 0.4986\n","Epoch 5/10\n","157/157 [==============================] - 15s 97ms/step - loss: 0.5381 - accuracy: 0.7204 - val_loss: 0.9385 - val_accuracy: 0.5020\n","Epoch 6/10\n","157/157 [==============================] - 15s 98ms/step - loss: 0.2724 - accuracy: 0.8765 - val_loss: 1.3821 - val_accuracy: 0.4920\n","Epoch 7/10\n","157/157 [==============================] - 16s 99ms/step - loss: 0.1444 - accuracy: 0.9364 - val_loss: 1.7770 - val_accuracy: 0.5074\n","Epoch 8/10\n","157/157 [==============================] - 15s 97ms/step - loss: 0.1266 - accuracy: 0.9426 - val_loss: 1.8512 - val_accuracy: 0.5106\n","Epoch 9/10\n","157/157 [==============================] - 15s 96ms/step - loss: 0.0920 - accuracy: 0.9582 - val_loss: 2.1711 - val_accuracy: 0.5086\n","Epoch 10/10\n","157/157 [==============================] - 15s 97ms/step - loss: 0.0671 - accuracy: 0.9693 - val_loss: 2.3509 - val_accuracy: 0.5046\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7fadb9c10df0>"]},"metadata":{},"execution_count":18}],"source":["# Ca overfit √† mort\n","# moins bon qu'hier\n","# le soucis vient de RNN\n","\n","model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715680871639,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"UPPIHNO0AZWh","outputId":"6675b910-b15b-423f-b943-ea62d62748aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text_vectorization (TextVe  (None, 100)               0         \n"," ctorization)                                                    \n","                                                                 \n"," embedding (Embedding)       (None, 100, 32)           320000    \n","                                                                 \n"," simple_rnn_3 (SimpleRNN)    (None, 100, 64)           6208      \n","                                                                 \n"," simple_rnn_4 (SimpleRNN)    (None, 32)                3104      \n","                                                                 \n"," dense (Dense)               (None, 16)                528       \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 329857 (1.26 MB)\n","Trainable params: 329857 (1.26 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# voir que le nombre params explose\n","\n","# Retrouver le nombre de params\n","# vocab_size      = 10_000\n","# sequence_length =    100\n","# embedding_dim   =     32\n","# batch           =    128\n","# n_units = nb de neurones = nomb colonne\n","# n_ligne = n_seq\n","\n","\n","# Text_vectorization\n","# Pas de param√®tre\n","\n","# Embedding\n","# On presente une vecteur de 10_000\n","# On sort une matrice de 100 x 32\n","# Parmas = 32 * 10_000 = 320_000\n","\n","# SimplRNN 3  - return sequence = True\n","# On pr√©sente une matrice 100 x 32\n","# On sort une matrice 100 x 64\n","# On a une 32 √† multiplier par un vecteur de 64\n","# Les ht-1 sont √† multipliers par les ht => 64 * 64\n","# 64 biais\n","# => 64 * (32 + 64 + 1)\n","# Params = 6_208 =  32 x 64 + 64 x 64 + 64 = 6_208\n","\n","# SimplRNN 4 - return sequence = False\n","# On pr√©sente une matrice 100 x 64\n","# On sort un vecteur 32\n","# Params = 3_104 =  32 x 32 + 32 x 64 + 32 = 3_104\n","\n","# dense\n","# On presente un vecteur de 32\n","# On sort un vecteur de 16\n","# Params = 16 * 32 + 16 = 528\n","\n","# dense_1\n","# On presente un vecteur 16\n","# On sort 1\n","# Params = 1 * 16 + 1 = 17\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"Q6IOQDso7z3a"},"source":["There is a lot of overfitting! Let's try with the other two types of layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVi3mCCT7_nq"},"outputs":[],"source":["# On change rien sauf GRU\n","\n","\n","embedding_dim=32 # the dimensionality of the representation space\n","\n","model = Sequential([\n","  vectorize_layer, # This layers encodes the string as sequences of int\n","  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # the embedding layer\n","  # the input dim needs to be equal to the size of the vocabulary + 1 (because of\n","  # the zero padding)\n","  GRU(units=64, return_sequences=True), # maintains the sequential nature\n","  GRU(units=32, return_sequences=False), # returns the last output\n","  Dense(16, activation='relu'), # a dense layer\n","  Dense(1, activation=\"sigmoid\") # the prediction layer\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsmCSFG76coy"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":635726,"status":"ok","timestamp":1715681507975,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"qtvnEGVP8d9G","outputId":"26fe91f7-bcfa-4d41-e1e1-47ce71ad9140"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","157/157 [==============================] - 47s 265ms/step - loss: 0.5901 - accuracy: 0.6498 - val_loss: 0.4150 - val_accuracy: 0.8124\n","Epoch 2/10\n","157/157 [==============================] - 46s 290ms/step - loss: 0.3473 - accuracy: 0.8537 - val_loss: 0.4180 - val_accuracy: 0.8170\n","Epoch 3/10\n","157/157 [==============================] - 39s 251ms/step - loss: 0.2630 - accuracy: 0.8989 - val_loss: 0.5066 - val_accuracy: 0.8218\n","Epoch 4/10\n","157/157 [==============================] - 43s 273ms/step - loss: 0.2158 - accuracy: 0.9190 - val_loss: 0.5235 - val_accuracy: 0.8204\n","Epoch 5/10\n","157/157 [==============================] - 48s 306ms/step - loss: 0.1781 - accuracy: 0.9379 - val_loss: 0.6196 - val_accuracy: 0.8014\n","Epoch 6/10\n","157/157 [==============================] - 45s 286ms/step - loss: 0.1526 - accuracy: 0.9485 - val_loss: 0.6271 - val_accuracy: 0.8010\n","Epoch 7/10\n","157/157 [==============================] - 42s 266ms/step - loss: 0.1216 - accuracy: 0.9614 - val_loss: 0.8267 - val_accuracy: 0.7978\n","Epoch 8/10\n","157/157 [==============================] - 47s 297ms/step - loss: 0.1030 - accuracy: 0.9668 - val_loss: 0.7425 - val_accuracy: 0.8010\n","Epoch 9/10\n","157/157 [==============================] - 44s 277ms/step - loss: 0.0736 - accuracy: 0.9778 - val_loss: 0.8191 - val_accuracy: 0.8006\n","Epoch 10/10\n","157/157 [==============================] - 49s 315ms/step - loss: 0.0652 - accuracy: 0.9818 - val_loss: 0.8391 - val_accuracy: 0.8006\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7fadb9c10e20>"]},"metadata":{},"execution_count":22}],"source":["# Compliquer n'am√©liore pas\n","\n","model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"8GW3s4AO9LXv"},"source":["Seems like using GRU instead of simpleRNN is helping the model a lot with the overfitting problem. Now let's compare this with LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYKGhY_v8fSE"},"outputs":[],"source":["\n","embedding_dim = 32 # the dimensionality of the representation space\n","\n","model = Sequential([\n","  vectorize_layer, # This layers encodes the string as sequences of int\n","  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # the embedding layer\n","  # the input dim needs to be equal to the size of the vocabulary + 1 (because of\n","  # the zero padding)\n","  LSTM(units=64, return_sequences=True), # maintains the sequential nature\n","  LSTM(units=32, return_sequences=False), # returns the last output\n","  Dense(16, activation='relu'), # a dense layer\n","  Dense(1, activation=\"sigmoid\") # the prediction layer\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnqAIATE9lHw"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678027,"status":"ok","timestamp":1715682187383,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"E70_hHes9nD5","outputId":"813133dc-9848-4b34-e522-411f85262f0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","157/157 [==============================] - 58s 344ms/step - loss: 0.5485 - accuracy: 0.6939 - val_loss: 0.4124 - val_accuracy: 0.8098\n","Epoch 2/10\n","157/157 [==============================] - 52s 334ms/step - loss: 0.3302 - accuracy: 0.8663 - val_loss: 0.4226 - val_accuracy: 0.8082\n","Epoch 3/10\n","157/157 [==============================] - 52s 330ms/step - loss: 0.2546 - accuracy: 0.9035 - val_loss: 0.4293 - val_accuracy: 0.8148\n","Epoch 4/10\n","157/157 [==============================] - 51s 326ms/step - loss: 0.2093 - accuracy: 0.9246 - val_loss: 0.4850 - val_accuracy: 0.8112\n","Epoch 5/10\n","157/157 [==============================] - 55s 352ms/step - loss: 0.1611 - accuracy: 0.9452 - val_loss: 0.5516 - val_accuracy: 0.8002\n","Epoch 6/10\n","157/157 [==============================] - 52s 331ms/step - loss: 0.1311 - accuracy: 0.9585 - val_loss: 0.7018 - val_accuracy: 0.7968\n","Epoch 7/10\n","157/157 [==============================] - 53s 335ms/step - loss: 0.1255 - accuracy: 0.9582 - val_loss: 0.6802 - val_accuracy: 0.8052\n","Epoch 8/10\n","157/157 [==============================] - 54s 344ms/step - loss: 0.0976 - accuracy: 0.9699 - val_loss: 0.7245 - val_accuracy: 0.7956\n","Epoch 9/10\n","157/157 [==============================] - 56s 359ms/step - loss: 0.0878 - accuracy: 0.9724 - val_loss: 0.7248 - val_accuracy: 0.7942\n","Epoch 10/10\n","157/157 [==============================] - 58s 370ms/step - loss: 0.0726 - accuracy: 0.9775 - val_loss: 0.7909 - val_accuracy: 0.7842\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7fadc8311ea0>"]},"metadata":{},"execution_count":25}],"source":["# N'am√©liore toujours pas encore\n","\n","# En effet, on fait de la classification\n","# Pas besoin d'analyser l'ordre des mots\n","# On peut faire du sentiment \"simple\" sans tenir compte de l'ordre des mots\n","\n","model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"yPBVeavM_1aM"},"source":["It looks like the results we obtain for GRU and LSTM are quite comparable, they are both able to solve the overfitting problem which is probably due to the fact that the input data consists in long sequences."]},{"cell_type":"markdown","metadata":{"id":"gAnpwz8mAczx"},"source":["## Conclusion\n","We conclude here that GRU and LSTM layers seem way better for supervised learning tasks than the simple RNN. If you are looking for other best practices for building recurrent neural network, this [blog post](https://danijar.com/tips-for-training-recurrent-neural-networks/) contains lots of great ideas for improving your results and getting better understanding overall of these types of models."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}