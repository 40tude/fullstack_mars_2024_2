{"cells":[{"cell_type":"markdown","metadata":{"id":"e4N9XyPIoWHV"},"source":["# <span style=\"color:red\"><b>TOURNE SUR PC</b></span>\n","# Code Encoder Decoder\n","\n","The goal of this demo is to teach you how to code an encoder decoder model!\n","Since this is just a demo we will use generated data, you'll be able to tackle the real problem during the exercise, the goal here is to focus on building the model and the training loop."]},{"cell_type":"markdown","metadata":{"id":"-CqUIHNG_w-j"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":214,"status":"ok","timestamp":1715770774810,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"PyeOCpH_yRRe"},"outputs":[],"source":["# Import Tensorflow & Pathlib librairies\n","import pathlib\n","import os\n","import io\n","import json\n","\n","import tensorflow             as tf\n","import pandas                 as pd\n","\n","from random                   import randint\n","from numpy                    import array\n","from numpy                    import argmax\n","from numpy                    import array_equal\n","from tensorflow.keras.utils   import to_categorical\n","from tensorflow.keras.models  import Model\n","from tensorflow.keras.layers  import Input\n","from tensorflow.keras.layers  import LSTM\n","from tensorflow.keras.layers  import Dense\n","from tensorflow.keras.utils   import plot_model\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8yZhjrVO_6aw"},"source":["## Generate data\n","\n","We will generate random input and target data for the purpose of the demonstration."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715770775051,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"wDwgQyLDHzbN"},"outputs":[],"source":["input_vocab_size       = 100           # nb de mots du vocabulaire\n","input_seq_len   = 10            # 10 mots dans les phrases FR\n","target_seq_len  = 5             # 5 mots dans les phrases US"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715770775052,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"0HQ-jiYSAD0D"},"outputs":[],"source":["# generate a sequence of random integers\n","def generate_sequence(length, n_unique):\n","\treturn [randint(1, n_unique-1) for _ in range(length)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1715772101520,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"w-N8q1gaBCsy","outputId":"56c2d880-7c9b-43bf-de70-209d47cc52fe"},"outputs":[],"source":["generate_sequence(input_seq_len, input_vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1715770775288,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"01pWZiHjAJFV"},"outputs":[],"source":["# faut pas de l'aléatoire\n","# faut faire comme si les phrases avaient du sens\n","# Source : on tire 10 int\n","# Target : on prend les 5 premiers de la source et on les retourne\n","\n","# prepare data for the LSTM\n","def get_dataset(n_in, n_out, cardinality, n_samples, printing=False):\n","  X1, X2, y = list(), list(), list()\n","  for _ in range(n_samples):\n","    # generate source sequence\n","    source = generate_sequence(n_in, cardinality)         # genere la source\n","    if printing:\n","      print(\"source:\", source)\n","    # define padded target sequence\n","    target = source[:n_out]\n","    target.reverse()\n","    if printing:\n","      print(\"target:\", target)\n","    # create padded input target sequence\n","    # c'est le teacher forcing\n","    # le zero c'est le start\n","    # voir que le dernier int de la target passe à la trappe\n","    # c'est pas gênant car il s'entraine à générer le mot d'après\n","    target_in = [0] + target[:-1]\n","    if printing:\n","      print(\"padded target:\", target_in)\n","    # store\n","    X1.append(source)\n","    X2.append(target_in)\n","    y.append(target)\n","  return array(X1), array(X2), array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1715770775289,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"rWkizklpBJpz","outputId":"cc9931ce-2a45-4a7a-c275-993e0bd4c48b"},"outputs":[],"source":["input, padded_target, target =  get_dataset(input_seq_len, target_seq_len, input_vocab_size, 1, True)"]},{"cell_type":"markdown","metadata":{"id":"T-6_AV5lD-6Y"},"source":["The data we are generating consists in a random sequence of numbers (they could very well represent encoded letters, words, sentences or anything you could think of).\n","\n","The target is built using the first elements of the input in reversed order.\n","\n","We also create a padded target sequence for teacher forcing (remember it is when the previous element from the target will be used as information for the decoder to predict the next element in the target)\n","\n","Now that we understand this, let's create the training data and validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715770775289,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"VeOFZcCeFFGj"},"outputs":[],"source":["X_train, padded_y_train, y_train  = get_dataset(input_seq_len,target_seq_len,input_vocab_size,10000)\n","X_val, padded_y_val, y_val        = get_dataset(input_seq_len,target_seq_len,input_vocab_size,5000)"]},{"cell_type":"markdown","metadata":{"id":"BPzJHFiSFaeo"},"source":["## Create encoder model\n","\n","In this step we will define the encoder model.\n","\n","The goal of the encoder is to create a representation of the input data, to extract information from the input data which will then be interpreted by the decoder model.\n","\n","The encoder receives sequence inputs and will output sequences with a given depth of representation (we  usually called that dimension channels before)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715770775290,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"0D_EKSjfLxd4"},"outputs":[],"source":["# Définir l'architecture de l'encoder c'est la partie simple\n","# C'est la même architecture en entrainement qu'en inference\n","# on travaille sur des int\n","# int dans une couche word embedding puis dans une couche LSTM\n","# et c'est tout\n","\n","# on figes les hyper parametres\n","\n","# let's start by defining the number of units needed for the embedding and\n","# the lstm layers\n","\n","n_embed = 32\n","n_lstm  = 16     # puissance de 2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":309,"status":"ok","timestamp":1715770780631,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"kquiEvuTHfYw"},"outputs":[],"source":["# l'architecture de l'encoder est séquentielle mais on ici s'entraine à le faire à la main\n","# On a l'habitude d'écrire des trucs du style :\n","# model = Sequential([\n","#   vectorize_layer,                                            # This layers encodes the string as sequences of int\n","#   Embedding(vocab_size, embedding_dim, name=\"embedding\"),     # the embedding layer the input dim needs to be equal to the size of the vocabulary + 1 (because of the zero padding)\n","#   SimpleRNN(units=64, return_sequences=True),                 # maintains the sequential nature\n","#   SimpleRNN(units=32, return_sequences=False),                # returns the last output\n","#   Dense(16, activation='relu'),                               # a dense layer\n","#   Dense(1, activation=\"sigmoid\")                              # the prediction layer\n","# ])\n","\n","\n","# Faut déclarer une instance Input où on fixe la taille de l'input\n","# input_seq_len = 10 mots par phrase\n","encoder_input = tf.keras.Input(shape=(input_seq_len), name=\"Encoder_In\")     \n","encoder_embed = tf.keras.layers.Embedding(input_dim = input_vocab_size, output_dim = n_embed, name=\"Encoder_WE\")\n","# pas oublier qu'on se fiche du output MAIS on veut h et C => return_state=True\n","encoder_lstm = tf.keras.layers.LSTM(n_lstm, return_state=True, name=\"Encoder_LSTM\")\n","\n","\n","\n","# CABLAGE DES COUCHES ###############\n","# Maintenant que les couches sont déclarées, on fait le câblage à la main des sorties et des entrées entre les couches\n","# Ici c'est séquentiel donc c'est très facile\n","encoder_embed_ouput = encoder_embed(encoder_input)\n","encoder_output      = encoder_lstm(encoder_embed_ouput)\n","\n","\n","\n","\n","# faut finir avec la classe Model et préciser ce qui sera inputs et outputs\n","encoder = tf.keras.Model(inputs = encoder_input, outputs = encoder_output)\n","\n","\n","plot_model(encoder)"]},{"cell_type":"markdown","metadata":{"id":"LxjlemPeKTt8"},"source":["That's it, it does not need to be anymore complicated than this, note though that we did not preserve the sequential nature of the data, but we output the cell state, which will serve as input state for the decoder!\n","\n","Let's try it out on an input to see what comes out!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715770867676,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"uzabOgSBoOEM","outputId":"91a127f8-a937-46d0-f3a5-39f5287f82da"},"outputs":[],"source":["X_train[0]\n","print(X_train[0])\n","\n","# On peut pas passer Xtrain directement\n","# Faut passer un batch\n","# d'où le expend dims\n","tf.expand_dims(X_train[0],0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1715771060511,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"XkgQs5mXJmHO","outputId":"251f69d1-c63c-4400-ae7a-557355fe95ec"},"outputs":[],"source":["# on vérifie que si on lui passe un tenseur tout se passe bien\n","encoder(tf.expand_dims(X_train[0],0))\n","\n","\n","# taille 16 car n_lstm = 16\n","# on recupere dans l'ordre\n","#   la dernière ligne\n","#   le hidden state   (= dernière ligne)\n","#   le cell state"]},{"cell_type":"markdown","metadata":{"id":"Q6Bd5XPCPrTJ"},"source":["## Create decoder\n","\n","* The goal of the decoder is to use the encoder output and the previous target element to predict the next target element\n","* Which means its output is a sequence with as many elements as the target \n","        * this is where the padded target comes in, it will serve as input \n","* It must have a number of channels equals to the number of possible values for target elements.\n","\n","We can't use the standard **Sequential** framework to build the model because the initial state of the decoder as to be set as the encoder states.\n","* In addition two versions of the same model (with the same weights) have to be prepared\n","        * one for training\n","        * one for inference (prediction on new unknown data). \n","        \n"]},{"cell_type":"markdown","metadata":{"id":"BhFnD8qQIUNQ"},"source":["### Decoder for training\n","\n","* Training the decoder requires that we use the teacher forcing mechanism\n","* We provide the correct answer from the previous element to predict the next element of the output sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715767409035,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"Km-Q0cMKsHD8","outputId":"520f0411-b268-4777-fcdf-f662a2a361aa"},"outputs":[],"source":["# on fait un test \n","# on récupere les vecteurs 1 et 2\n","# c'est les h et C de l'encoder\n","encoder_output[1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"executionInfo":{"elapsed":1191,"status":"ok","timestamp":1715780618843,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"fgWYnH-UQU1Y","outputId":"37d49ca7-58fb-4d32-a7b0-063b48d7ffaa"},"outputs":[],"source":["# pour le decodeur il y a 2 modes de fonctionnement\n","#     entrainement = teacher forcing\n","#     inference    = un token à la fois\n","\n","\n","\n","# DECLARATION DES COUCHES ###############\n","\n","# Ici on fait le cablage pour l'entrainemnt\n","# en entrainement target_seq_len = 5 car les phrases UK ont 5 mots\n","# cet input va dans WE, LSTM puis dans la couche dense avant la sortie\n","\n","# On rentre un vecteur de 5 car on veut des phrase en UK de 5 mots\n","decoder_input = tf.keras.Input(shape=(target_seq_len), name=\"Decoder_Input\") # target_seq_len = 5\n","\n","# On met une couche Word Embedding car le LSTM peut pas prendre un token en direct\n","# Va sortir une matrice 5 x 32\n","decoder_embed = tf.keras.layers.Embedding(input_dim = input_vocab_size, output_dim = n_embed, name=\"Decoder_WE\") # imput_dim = 100 (taille du vocabulaire)   n_embed = 32\n","\n","# on met le 5 x 32 dans le LSTM\n","# 16 units pour le LSTM\n","# return_sequences et return_state sont à True\n","# Donc on sort 5 x 16 + hidden(16) + cell state(16)\n","decoder_lstm = tf.keras.layers.LSTM(n_lstm, return_sequences=True, return_state=True, name=\"Decoder_LSTM\") # n_lstm = 16\n","\n","\n","\n","\n","# le 5 x 16 rentre dans une couche dense\n","# 100 neurones et softmax\n","# Y a 100 neurones car c'est la taille du vocabulaire et que le soft max va désigner le mot le plus probable\n","\n","# TF permet de brancher une couche dense sur une matrice\n","# En sortie on aura 1 x 100\n","# Soft max => somme à 1\n","# On regarde le argmax de chaque ligne en fait\n","decoder_pred = tf.keras.layers.Dense(input_vocab_size, activation=\"softmax\", name=\"Decoder_Softmax\") # input_dim = 100 mots du vocabulaire\n","\n","\n","\n","\n","\n","# CABLAGE DES COUCHES ###################\n","\n","\n","\n","# Entrée TEACHER FORCING\n","# C'est ici qu'on indique qu'il faut réutiliser\n","# the decoder input is actually the padded target we created earlier, remember\n","# if target is              :    [91, 47, 89, 21, 62]\n","# the decoder input will be : [0, 91, 47, 89, 21]\n","# teacher forcing happens here\n","decoder_embed_output = decoder_embed(decoder_input)\n","# decoder_embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"decoder_embedding\")\n","# decoder_embed_output = decoder_embed(decoder_input)\n","\n","# Bien voir le initial_state qui vaut encoder_output[1:]\n","# encoder_output[1:] => on recupere h et C\n","decoder_lstm_output, _, _ = decoder_lstm(decoder_embed_output, initial_state = encoder_output[1:])\n","\n","# in the step described above the decoder receives the encoder state as its\n","# initial state.\n","decoder_output = decoder_pred(decoder_lstm_output)\n","# then the dense layer will convert the vector representation for each element\n","# in the sequence into a probability distribution across all possible tokens\n","# in the vocabulary!\n","\n","\n","# Bien voir qu'on indique que le model accepte 2 types d'entrée encoder_input ou decoder_input\n","encoder_decoder = tf.keras.Model(inputs = [encoder_input, decoder_input], outputs = decoder_output)\n","\n","\n","\n","\n","\n","\n","\n","\n","plot_model(encoder_decoder, show_shapes=True, show_dtype=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyquXF4aX-aM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gELMttoHbqOx"},"source":["Let's try out the decoder model on some input sequences!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1715767428724,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"en8cZTDpbwrc","outputId":"79c696d9-c71e-4444-d49f-db7ad92ff9ca"},"outputs":[],"source":["# la somme de la premiere ligne de 100 fait 1\n","# c'est le token qui a la valeur max qui est la prédiction\n","encoder_decoder([tf.expand_dims(X_train[0],0),tf.expand_dims(padded_y_train[0],0)])"]},{"cell_type":"markdown","metadata":{"id":"Zyg7N6F3KSgX"},"source":["### Decoder for inference (prediction)\n","\n","Contrary to the training case, for inference we do not have access to the target nor the padded target. The decoder input will be made out of a sequence starting with $0$ which is the special start token in our case, then followed by the predictions of the decoder as they come."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":1113,"status":"ok","timestamp":1715780233011,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"AgnWHaCFKzm-","outputId":"886af434-d78e-46e4-bcaf-bcfa5413039a"},"outputs":[],"source":["# On va changer le cablage mais pas les couches\n","# On ne fait donc que déclarer des inputs\n","# on garde les couches\n","\n","\n","# DECLARATION DES COUCHES ###############\n","\n","# En inference l'input c'est plus une taille de 5 mais 1 token à la fois\n","# un token => vect de taille 1\n","decoder_input_inf = tf.keras.Input(shape=(1), name = \"Inference_in\")\n","\n","\n","# Au premier tour on alimente le decoder avec les h et C de l'encoder\n","# for following steps, they will become the hidden and C state from the decoder itself\n","# since the input sequence is unknown we will have to predict step by step using a loop\n","decoder_state_input_h = Input(shape=(n_lstm,), name=\"Hidden_state\")\n","decoder_state_input_c = Input(shape=(n_lstm,), name=\"Cell_state\")\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","\n","\n","\n","\n","\n","\n","# CABLAGE DES COUCHES ###################\n","\n","\n","decoder_embed_output = decoder_embed(decoder_input_inf)\n","# the decoder input here is of shape 1 because we will feed the elements in the\n","# sequence one by one\n","\n","\n","# Bien voir le initial_state qui vaut decoder_states_inputs\n","# en sortie 1x100\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_embed_output, initial_state=decoder_states_inputs)\n","# the lstm layer works in the same way, the output from the embedding is used\n","# and the decoder state is used as described above\n","\n","decoder_states = [state_h, state_c]\n","# we store the lstm states in a specific object as we'll have to use them as\n","# initial state for the next inference step\n","\n","decoder_outputs = decoder_pred(decoder_outputs)\n","# the lstm output is then converted to a probability distribution over the\n","# target vocabulary\n","\n","# Finally we wrap up the model building by setting up the inputs and outputs\n","# En mode inférence il va sortir 2 choses\n","#     - ses prédictions\n","#     - ses états internes : ce sont ces états qu'il réutlisera pour traduire le mot d'après\n","decoder_inf = Model(inputs   = [decoder_input_inf, decoder_states_inputs],\n","                    outputs  = [decoder_outputs, decoder_states])\n","\n","plot_model(decoder_inf, show_shapes=True, show_dtype=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"r3XHFd6YdgXQ"},"source":["Here we'll give you an example of how this version of the model will be able to give predictions, we'lls need to write a loop for this!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1715767894228,"user":{"displayName":"Philippe BAUCOUR","userId":"15978005709614721222"},"user_tz":-120},"id":"5JS5yQyxd107","outputId":"89a1d225-70ee-4bf7-88ba-4264cc1d1c0f"},"outputs":[],"source":["# on test ce qui se passe si on passe un \"start\"\n","enc_input = tf.expand_dims(X_train[0],0)\n","#classic encoder input\n","\n","\n","# on passe un 0 0 = un token start\n","dec_input = tf.zeros(shape=(1,1))\n","# the first decoder input is the special token 0\n","\n","enc_out, state_h_inf, state_c_inf = encoder(enc_input)\n","# we compute once and for all the encoder output and the encoder\n","# h state and c state\n","\n","# on va utilser dec_state pour initiliaser plus tard\n","dec_state = [state_h_inf, state_c_inf]\n","\n","# we'll store the predictions in here\n","pred = []  \n","\n","# we loop over the expected length of the target, but actually the loop can run\n","# for as many steps as we wish\n","# which is the advantage of the encoder decoder\n","# architecture\n","# target_seq_len = 5\n","for i in range(target_seq_len):\n","  dec_out, dec_state = decoder_inf([dec_input, dec_state])\n","  print(dec_out)\n","  print(\"----------------\")\n","\n","  # the decoder state is updated and we get the first prediction probability\n","  # vector\n","  decoded_out = tf.argmax(dec_out, axis=-1)\n","  # we decode the softmax vector into and index\n","  pred.append(decoded_out) # update the prediction list\n","  dec_input = decoded_out # the previous pred will be used as the new input\n","\n","pred"]},{"cell_type":"markdown","metadata":{"id":"zBbC19IPxGju"},"source":["## Training the encoder decoder model\n","\n","We are almost there, the difficult part of this was building the model, now the training step will be super easy!\n","All we have to do is first `compile` the model to assign a loss function then use the `fit` method!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X6jktGKyIok"},"outputs":[],"source":["# Faut entrainer et tester\n","# La loss c'est la même pour les 2 (encoder, decoder)\n","# Sparce matrix car on donne la target sous forme de vecteur\n","\n","# SparseCategoricalCrossentropy est utilisée dans les cas où les étiquettes de classe sont fournies sous forme d'entiers\n","# Par opposition à des vecteurs one-hot.\n","# Adaptée aux problèmes de classification où chaque exemple d'entraînement appartient à une seule classe.\n","# Lorsque vous utilisez cette fonction de perte, il est attendu que les étiquettes de classe soient représentées par des entiers, et non par des vecteurs one-hot.\n","# Si vous avez trois classes (par exemple, 0, 1 et 2), les étiquettes doivent être des entiers dans cet intervalle, et non des vecteurs one-hot (par exemple, [1, 0, 0], [0, 1, 0], [0, 0, 1]).\n","# Vous utilisez SparseCategoricalCrossentropy comme fonction de perte, ce qui signifie que vos étiquettes de classe sont représentées par des entiers.\n","# Cela correspond également au choix de la métrique SparseCategoricalAccuracy pour évaluer les performances de votre modèle.\n","\n","encoder_decoder.compile(\n","    optimizer=\"Adam\",\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),    # sparce car il sort\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")\n","\n","# On monitor l'accuracy du model de p\n","# Il sort 70% de accuracy\n","# Cest vraiment pas mal mais mode débile il a une chance sur 100 de sortir le bon mot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176142,"status":"ok","timestamp":1633611154025,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"XBKz1jT-xTA3","outputId":"1a87a041-0050-4050-9e13-df005fab81d3"},"outputs":[],"source":["encoder_decoder.fit(x=[X_train,padded_y_train],y=y_train,epochs=50, validation_data=([X_val,padded_y_val],y_val))"]},{"cell_type":"markdown","metadata":{"id":"JaF316p_02jy"},"source":["Nice! The training is over, and it looks as though we could have continued to train the model even longer since it has not yet started to overfit!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2Z6VfnKxkO_"},"outputs":[],"source":["# plot_model()\n","# 3 et 4 c'est h stat et C stte"]},{"cell_type":"markdown","metadata":{"id":"l4C6vHoo1NqW"},"source":["## Make predictions with the inference model\n","\n","I don't know if you have noticed, but we used the exact same layers for the training and the inference model, therefore they have the same weights, only we are able to use the inference model on new data since we cannot use teacher forcing anymore!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1633611934800,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"kgymx0a41s-g","outputId":"67ebb9e7-8023-4be4-dba2-fbf15b083c37"},"outputs":[],"source":["# On fait un test sur le val set car on connait ce que l'on veut\n","\n","enc_input = X_val\n","#classic encoder input\n","\n","dec_input = tf.zeros(shape=(len(X_val),1))\n","# the first decoder input is the special token 0\n","\n","\n","# On passe tous les X val dans l'encoder\n","# On recupere h et C\n","enc_out, state_h_inf, state_c_inf = encoder(enc_input)\n","# we compute once and for all the encoder output and the encoder\n","# h state and c state\n","\n","\n","dec_state = [state_h_inf, state_c_inf]\n","# The encoder h state and c state will serve as initial states for the\n","# decoder\n","\n","pred = []  # we'll store the predictions in here\n","\n","\n","\n","# On a 5 mots => 5 iteration\n","# we loop over the expected length of the target, but actually the loop can run\n","# for as many steps as we wish, which is the advantage of the encoder decoder\n","# architecture\n","for i in range(target_seq_len):\n","  # ! au 1er tour on passe le dec_sta de l'encoder\n","  # Au 2me tour on passe le dec_state du decoder et plus de l'encoder\n","  dec_out, dec_state = decoder_inf([dec_input, dec_state])\n","  # the decoder state is updated and we get the first prediction probability vector\n","  decoded_out = tf.argmax(dec_out, axis=-1)\n","  # we decode the softmax vector into and index\n","  pred.append(decoded_out) # update the prediction list\n","  dec_input = decoded_out # the previous pred will be used as the new input\n","\n","\n","# Bien voir que quand il commence à se tromper il se trompe pour tout le reste\n","# Si il se trompe dès le départ tout\n","pred = tf.concat(pred, axis=-1).numpy()\n","for i in range(10):\n","  print(\"pred:\", pred[i,:])\n","  print(\"true:\", y_val[i,:])\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"SN9kGOmC3E57"},"source":["The results do not look so bad, however it looks as though once the model make a mistake on one of the predictions, then the rest of the sequence will also not be well predicted!\n","\n","This behaviour can be explained in the following way: the information taken from the encoder is only taken into account directly in the first decoding step, which means that everything that happens after this step depends on what information the decoder feeds itself from that point onwards.\n","\n","The encoder decoder framework however has made possible major advances, especially in terms of predicting sequences of arbitrary length. However we'll learn tomorrow about a solution that can deal with the \"worsening of predictions over the sequence\" problem!\n","\n","I hope you found this demonstration useful! Now it is time for you to apply what you have learned to a real world automatic translation problem!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
