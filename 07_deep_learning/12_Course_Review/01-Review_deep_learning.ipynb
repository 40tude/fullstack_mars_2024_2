{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Review\n",
    "\n",
    "We've seen a lot of things in Deep Learning, let's spend a little time on the theory.\n",
    "\n",
    "You can start by watching this video: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div className='aspect-w-16 aspect-h-9'><iframe src='https://www.youtube.com/embed/FD4ZpdfbkXU' allow='autoplay; fullscreen; picture-in-picture' allowFullScreen='' width='100%' title='Test' frameBorder='0'></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for this course\n",
    "\n",
    "Come with questions about things you didn't understand or want to know more about.\n",
    "\n",
    "\n",
    "## The main theoretical elements to remember\n",
    "\n",
    "### The different neural networks\n",
    "\n",
    "Deep Learning is based on the use of **Neuron Networks**. We started with simple networks called the **perceptron**.\n",
    "\n",
    "Deep Learning has evolved particularly in two areas:\n",
    "\n",
    "1. Image analysis --> Object detection and image classifications\n",
    "2. Analysis of texts --> Sentiment analysis, translators, etc.\n",
    "\n",
    "This is why two evolutions of neural networks have been discovered:\n",
    "\n",
    "1. The **Convolutional Neuron Networks** (CNN)\n",
    "2. The **Recursive Neuron Networks** (RNN)\n",
    "\n",
    "The first ones work with a convolution matrix that will determine the contours of an image and bring out the most important pixels. This technique greatly helps the algorithm to understand the images.\n",
    "\n",
    "The second works with a _memory_. Recursive neural networks take past occurrences to make future predictions. This is extremely useful for creating text-generating algorithms since we will use the previous word to determine the next word in the sentence.\n",
    "\n",
    "\n",
    "### How neural network training works\n",
    "\n",
    "To make a neural network work, you need:\n",
    "\n",
    "1. Define a **network** (CNN / LSTM / Dense)\n",
    "2. Apply a **forward pass** --> What are the calculations that the neurons will do to move from one layer to another?\n",
    "3. Define a **cost function** (MSE, CrossEntropy, SparseCrossEntropy etc.)\n",
    "4. Apply a **backpropagation** by calculating the gradients of the cost function.\n",
    "5. 5. Repeat the operation for each dataset observation on a defined number of epochs.\n",
    "6. In this way you will find the minimum of your cost function.\n",
    "\n",
    "### Deep Learning on TensorFlow 2.0\n",
    "\n",
    "FastAI has the shortcomings of its quality: its extreme practicality is paid for by a lack of flexibility. This is why it is useful to know about other Deep Learning libraries such as TensorFlow 2.0.\n",
    "\n",
    "Do not forget:\n",
    "\n",
    "1. Use Transfer Learning as much as possible\n",
    "2. Transform everything into Tensors (image + text) and integrate them into a `tf.data.Dataset`.\n",
    "3. That most of the time, you will be able to use Keras directly to solve most Deep Learning problems.\n",
    "4. That if you need more flexibility, you can use subclasses to define your templates and cost functions yourself.\n",
    "\n",
    "Remember also the GANs as the new algorithms we have been able to implement on TensorFlow which are based on the following principle:\n",
    "\n",
    "* We have two algorithms: A **generator**, a **discriminator**\n",
    "* The purpose of the **generator** is to create the most accurate images possible.\n",
    "* The purpose of the **discriminator** is to differentiate between real and dummy images\n",
    "* The model trains on the principle that the **generator** can _deceive_ the **discriminator**\n",
    "\n",
    "### Things you need to focus on when setting up a neural network\n",
    "\n",
    "There's a lot of stuff to put in place when you build your algorithm. Here is a summary of the things you should think about to debug if you need to:\n",
    "\n",
    "* Your pre-processing is key. If you manage to insert your tensors into a `tf.data.Dataset', that's often a good sign. However, be really clear about your variables and your target.\n",
    "\n",
    "* When you create your structure, don't forget to take into account the size of your dataset and the *overfitting* potential of your model. So put some _Dropout_ between each hidden layer.\n",
    "\n",
    "* Your activation functions are important. Use ReLu or leakyRelu for the hidden layers. For outputs, use either `sigmoid' for binary classification, or `softmax' for multiple classification. \n",
    "\n",
    "* Take a good look at the cost functions. You most often use **MSE** for regression problems and **CrossEntropy** for classification problems.\n",
    "\n",
    "* Don't forget to put a **Learning Rate Schedule**. It is indeed important that your Learning Rate decreases as you train your model.\n",
    "\n",
    "* When you set up an optimizer, Adam is now the most efficient of all.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
