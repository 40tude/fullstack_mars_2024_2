{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is an algorithm that helps \"climb down a mountain\" all the way to a relative or absolute bottom. The mountain is the loss function, and the bottom of the valley is a minimum (local or global).\n",
    "\n",
    "Gradient : vector, of the partial derivatives of the loss function according to the model's parameters. The direction of the gradient corresponds to the direction of maximum increase of the loss function (which is why going the opposite way decreases the loss function). \n",
    "\n",
    "Gradient descent algorithm:\n",
    "* Initialisation: give a random (or not) initial value to the model's parameters\n",
    "* Iteration: $$\\beta^{t+1} = \\beta^{t} - \\gamma \\nabla_{\\beta} C$$\n",
    "* Stopping: \n",
    "    * if the gradient norm falls under a certain value (convergence)\n",
    "    * number of iteration\n",
    "    * if the validation loss starts increasing (overfitting limit)\n",
    "\n",
    "## +\n",
    "* Calculation is easy (chain rule) (computation efficient)\n",
    "* Always brings us in the right direction for training the parameters on the train set\n",
    "\n",
    "## -\n",
    "* local minimum (Initialisation)\n",
    "* Explosive gradients\n",
    "* Vanishings gradients\n",
    "* Dying relu\n",
    "* Final model is very sensitive to the initial values of the parameters\n",
    "\n",
    "## Batch and Stochastic gradient descents\n",
    "* stochastic : random subset of observations at each step\n",
    "* batch : split the data into random batches -> 1 epoch is one sweep through all the batches\n",
    "\n",
    "The advantage is to make more updates of the parameters with less computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected (dense) neural networks\n",
    "\n",
    "A dense neuron :\n",
    "* w (one parameter per input value)\n",
    "* b a bias\n",
    "* activation function\n",
    "    * ReLu:\n",
    "        * \\+ fast computation\n",
    "        * \\- Dying relu (derivative is 0 for negative inputs)\n",
    "    * Leaky ReLu\n",
    "        * \\+ same as ReLu minus the dying part\n",
    "    * Sigmoid:\n",
    "        * \\+ gives you a probability (good for binary classification or multi-modal classification) \n",
    "        * \\+ smooth gradients\n",
    "        * \\- vanishing gradients (because the function is saturating for negative and positive values)\n",
    "        * \\- computation time\n",
    "    * Softmax:\n",
    "        * \\+ categorical classification probabilities\n",
    "    * Tanh:\n",
    "        * \\+ nice properties for the second derivative -> used in RNN\n",
    "        * \\- like sigmoid\n",
    "    * Swish:\n",
    "        * \\+ google's tried it and works good (smooth ReLu) $x \\times \\sigma$\n",
    "\n",
    "A fully connected (dense) neuron is connected to each element of its inputs, one parameter per input value.\n",
    "Reads the entire input on the first layer (creating new features from all the data at step one like base ingredients)\n",
    "\n",
    "## Architecture guidelines\n",
    "* Start with lots of neurons then decrease (lots of base ingredients to make lots of recipes)\n",
    "* Generally up to three layers total (after that no better results but more overfitting, but that's empirical)\n",
    "* Last layer should be compatible with the target:\n",
    "    * Regression, y quantitative -> 1 neron with linear activation\n",
    "    * Binary classification -> 1 neuron sigmoid activation\n",
    "    * Categorical classification -> 1 neuron per category activation softmax\n",
    "    * Multi label classification -> 1 neuron per category activation sigmoid\n",
    "* First layer -> specify the input dimension\n",
    "\n",
    "Effect of adding more neurons in one layer is: exploring more possibilities of features at one level of complexity/non-linearity. Effect of adding more layers: increase the level of attainable non-linearity.\n",
    "\n",
    "## Prepare for training\n",
    "* compile :\n",
    "    * choose the loss function:\n",
    "        * MSE, MAE, MAPE -> Regression\n",
    "        * (Sparse)CategoricalCrossEntropy -> Classification\n",
    "        * BinaryCrossEntropy -> Binary Classification\n",
    "    * Optimizer (choice of algorithm) -> Adam (adapts the learning rate depending on the gradient values, if gradient increases-> slows down, when gradient diminishes -> accelerates, like a skier)\n",
    "    * Metrics (more interpretable than the loss)\n",
    "        * MSE MAE MAPE\n",
    "        * Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Image porcessing\n",
    "* normalize pixel values to be in the range 0->1 (dividing by 255)\n",
    "* images need to have shape (batch_size, width, heigth, channels=(1,3,4)) 1 B&W 3 RGB 4 RGBA (A is transparency)\n",
    "* Data Augmentation: any transformation of your data that let's you artificially increase the size of the dataset without altering the target (reasonnable with regards to the problem)\n",
    "* use ImageDataGenerator from tensorflow (easy data augmentation & no need to load all data in memory)\n",
    "\n",
    "## convolutional neuron\n",
    "* Kernel size: window that will travel on the input -> the size of the patterns you'll be detecting on the input, it also defines the number of parameters \n",
    "* padding: do we add zero padding on the edges of the input -> the goal is to be able to look at the corners and borders of the input + maintain the size of the original input\n",
    "* Strides: The way the kernel is going to travel on the input, does it stop at every element or every 2 element etc...\n",
    "* activation function (ReLu in most cases)\n",
    "\n",
    "## Architecture guidelines\n",
    "* Start with a few neurons then increase the number of neurons (first layers capture small patterns -> subsequent layers capture bigger and bigger patterns that allow for more and more variety, so we need more neurons to catch them all)\n",
    "* Deeper = Better (limited by the input shape, it's not worth capturing patterns that are bigger than the input)\n",
    "* Alternate between convolutional layers and pooling layers to gradually reduce the size of the input\n",
    "* Before prediction we need to flatten the last convolution output before feeding it to dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## General Principle\n",
    "* Pretrained model trained on a huge dataset that is similar to yours\n",
    "* \\+ save time and ressources \n",
    "* \\+ achieve better results with minimum effort and cost\n",
    "* \\+ take advantage of performance to train on small datasets\n",
    "* \\- data must be similar to pretraining data (medical images not compatible with daily life images)\n",
    "\n",
    "## Architecture guidelines\n",
    "* Remove the last layer to replace it with a layer adapted to your problem\n",
    "* If you want to work with different level of pattern complexity:\n",
    "    * Resnet (conv net with additive bypassing)\n",
    "    * Densenet (conv net with concatenate bypassing)\n",
    "    * Inception (using different convolution layers in parallel)\n",
    "* Finetuning: letting more top layers train to further adapt the model to our problem\n",
    "\n",
    "## Where to find them?\n",
    "* Tensorflow API (imagenet)\n",
    "* Tensorflow Hub\n",
    "* GitHub (paper with code)\n",
    "* Google Scholars (find scientific paper and manually reproduce the architecture)\n",
    "* Hugging Face (pretrained model and preset architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gans\n",
    "\n",
    "## General intuition\n",
    "* mafia (Generator) vs. cops (Discriminator)\n",
    "* Generator produces fake data from noise or input data\n",
    "* Discriminator separates fake data from real data\n",
    "* Unsupervised (do not need labels, just need data examples)\n",
    "\n",
    "## Usecases\n",
    "* Generate anonymous data\n",
    "* Complete missing data, missing values\n",
    "* Produce realistic fakes\n",
    "* Anomaly detection\n",
    "* Domain adaptation (super resolution)\n",
    "* Image translation (google maps into landscape)\n",
    "\n",
    "## Architecture guidelines\n",
    "* Conv transpose layers (inverse of convolution)\n",
    "* stride of 2 is better than pooling of inverse pooling\n",
    "* Leaky ReLu\n",
    "* Batchnormalization\n",
    "* generator: more units at the beginning than at the end\n",
    "* penalty on discriminator\n",
    "* noisy labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "## General principle\n",
    "* represent text with numbers\n",
    "* keep the sequential aspect of data\n",
    "* represent each word by a vector\n",
    "* embedding vectors represent a summarized meaning of the word (either connected to the target or to similar words in the language -> word2vec)\n",
    "\n",
    "## Training the embedding\n",
    "\n",
    "### Supervised training\n",
    "* Use the embedding to predict a target variable, so the embedding parameters will be trained to represent the words according to their connection to the target (example: cat and airplane could both be connected to good movie reviews, although they are not similar words)\n",
    "\n",
    "### Unsupervised training\n",
    "Word2Vec\n",
    "* Bag of words: Taking a group of words and try to predict the word in the middle\n",
    "* Skip Gram: taking one word and associate it positively with words that are close and negatively to random words (positive vs. negative skigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "## General Principle\n",
    "* Read through a sequence of inputs while keeping a memory of the previous words in the sequence\n",
    "* Simple RNN bad (vanishing gradient -> there is no choice but to go through the tanh activation)\n",
    "* memory is persisted in the form of a hidden state vector\n",
    "\n",
    "## GRU (gated recurrent unit)\n",
    "* reset gate: choose whether to use the current input for creating the new output or not\n",
    "* update gate: \n",
    "    * produce the new output\n",
    "    * decide wether to replace the old output with the new one or not\n",
    "* possibility to bypass the tanh activation -> less vanishing gradient\n",
    "\n",
    "## LSTM (long short term memory)\n",
    "* forget gate: do you erase previous memory of not\n",
    "* input gate:\n",
    "    * sigmoid: are we going to use the new information in the memory?\n",
    "    * tanh: what is the new information?\n",
    "    * feeds the portion of the new info in the memory\n",
    "* output gate: choosing what portion of the memory to use as the output (hidden state)\n",
    "* hidden state and cell state\n",
    "* possibility to bypass the tanh -> less vanishing gradient\n",
    "\n",
    "## Architecture guidelines\n",
    "* not several RNN layers after one another\n",
    "* same guidelines as for dense networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder\n",
    "\n",
    "## General principle\n",
    "* allowing to deal with output of arbitrary length\n",
    "* Teacher forcing (feed the right previous answers at each step)\n",
    "\n",
    "## Attention\n",
    "* Instead of looking at the encoder output only once with the decoder, at each step we assign importance weights to the encoder output in order to prioritize the information to be used\n",
    "* partly solves the error propagation problem of the encoder decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
