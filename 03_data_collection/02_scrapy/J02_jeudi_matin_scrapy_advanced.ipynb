{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy advanced\n",
    "\n",
    "In this course we will dig further into scrapy's capabilities, which are numerous, and good enough to let you scrape even websites that are attempting to protect themselves from this practice (for example sites like google or facebook are particularly reluctant to being scraped because they do not want other people being able to leverage their users' data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "This course will introduce the following concepts:\n",
    "\n",
    "* Callbacks\n",
    "* Web navigation with scrapy\n",
    "* Post requests with scrapy\n",
    "* Multiple spiders\n",
    "* Scrapy Projects\n",
    "* Avoid being banned\n",
    "    * Scrapy projects\n",
    "    * Autothrottle\n",
    "    * Rotate user agent\n",
    "    * Rotate IP address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are a way to call functions in our code at specific moments or when a specific task is done, depending on contraints and events that are partly or fully independent from the code itself. For example we may want to use callbacks when:\n",
    "\n",
    "* We want a function to run when a user clicks a certain button on an app\n",
    "* We want a function to run when the response from the webserver has been fully loaded\n",
    "* We want a function to run when a user lands on a specific url\n",
    "\n",
    "The list goes on.\n",
    "Scrapy uses a lot of callbacks for tasks that repeat over different pages, or if we want scrapy to follow link after link after link to explore a website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following pagination links üìÑüìÑ\n",
    "\n",
    "Scrapy simulates a web browser that will send requests to web servers, therefore it can do almost everything a web browser can do, such as naviagting the web using links! All you have to do is get the XPath from the link you are looking to click and use the `.follow` method.\n",
    "[This code example](src/scrapy3.py) shows how to use links to iterate over multiple pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/scrapy3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from this scraping process may be found in [this file](src/3_quotesmultiplepages.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication on a website üîêüîê\n",
    "\n",
    "A very useful feature of Scrapy: you can simulate automatic authentication! More generally, you may fill out forms and submit them like any normal web users would!\n",
    "\n",
    "This can be done by using `scrapy.FormRequest.from_response()` to send a post request with some your login/password to the website.\n",
    "\n",
    "This method will look for a `<form>` tag in the response object (the html code of the url you requested) and will populate the different fields with data of your choice. The data should be given to the `formdata` argument in the form of a dictionnary `{key:value}` where the `key` is the id attribute of the form field and `value` is what you wish to write there. Automatically this method will look for any clickable or submit button in the form and click.\n",
    "\n",
    "You can take a look at the [documentation](https://docs.scrapy.org/en/latest/topics/request-response.html#formrequest-objects) to get a better understanding.\n",
    "\n",
    "You may see this in [this script](src/scrapy4.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/scrapy4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this query is available in [this file](src/4_quotesauthentication.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run spiders with multiple starting urls üï∏Ô∏è üï∑Ô∏è\n",
    "\n",
    "It is possible to run a spider using several starting urls. Take a look at the [code example](src/scrapy5.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/scrapy5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may take a look at the [result file](src/5_quotesmultiplespiders.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid being banned\n",
    "\n",
    "Several websites are trying to protect themselves from scrapers because they do not want their data (which has a lot to do with their added value in most cases) to be captured by other people. This part of the lecture will focus on using scrapy's capabilities that make it harder to detect and therefore harder to be stopped by reluctant websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy projects\n",
    "\n",
    "Most of the techniques that can make your scraping processes harder to detect have nothing to do with the actual scraping code, that entirely depends on the webpage you are trying to scrape and its source code, it has everything to do with the configuration of everything that surrounds your spider.\n",
    "\n",
    "In order to easily access the configuration files of the scraping process and set up everything painlessly we can use what we call scrapy projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"src\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the scrapy project will appear in your current working directory:\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ tutorial\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tutorial\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spiders\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ scrapy.cfg\n",
    "```   \n",
    "  \n",
    "We will define the spiders we wish to use to scrape the web in the `spiders` folder. The python files and the `cfg` file we will use to configure the way the spider script will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autothrottle üö´üö´\n",
    "\n",
    "The more scraping you're doing the more requests you make. If websites are well protected, they might ban you because you exceeded requests limitations. \n",
    "\n",
    "You may avoid that by delaying the number of requests automatically thanks to the `AutoThrottle` extension. \n",
    "\n",
    "As stated in the documentation, `AutoThrottle` extension is designed to: \n",
    "\n",
    "- *Be nicer to sites instead of using default download delay of zero.*\n",
    "- *Automatically adjust Scrapy to the optimum crawling speed, so the user doesn‚Äôt have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.*\n",
    "\n",
    "To use autothrottle, it's as simple as adding `\"AUTOTHROTTLE_ENABLED\": True` to your crawler's settings. If you are working with scrapy projects (as you should if you are trying to do some intense scraping) you only need to uncomment the appropriate line in the [settings.py](src/tutorial/tutorial/settings.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"tutorial\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch a spider contained in a scrapy project, all you have to do is `cd` into the project directory (in our case `tutorial`) then run the following command:\n",
    "\n",
    "`!scrapy crawl name_of_the_spider -O filename_to_save_the_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl autothrottle -O results/6_quotesautothrottle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate the user agent\n",
    "\n",
    "In some cases, websites do not expect the same browser to make multiple requests in a short amount of time, this is why rotating user agents can be useful not to get caught scraping by the website.\n",
    "\n",
    "All you will have to do is install the `scrapy-user-agents` library and then include this code in the `settings.py` file:\n",
    "\n",
    "```python\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy-user-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject rotate_user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"rotate_user_agent\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl rotate_user_agent -O results/rotate_user_agent.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate IP addresses\n",
    "\n",
    "A very easy way to detect scrapers is to count the number of calls made by the same IP address in a given amount of time. An easy solution for this is to rotate between proxies that will act as an intermediary between your web client and the web server, making it see IP addresses that are not yours.\n",
    "\n",
    "First we need to install the `scrapy-rotating-proxy` library.\n",
    "\n",
    "All you have to do is find a list of proxies and add it to the `settings.py` file in the following way:\n",
    "\n",
    "```python\n",
    "ROTATING_PROXY_LIST = [\n",
    "    'Proxy_IP:port',\n",
    "    'Proxy_IP:port',\n",
    "    # ...\n",
    "]\n",
    "```\n",
    "\n",
    "Or you could also get the proxy list from file:\n",
    "\n",
    "```python\n",
    "ROTATING_PROXY_LIST_PATH = 'listofproxies.txt'\n",
    "```\n",
    "\n",
    "We'll use proxies given in the [proxy list file](src/Free_Proxy_List.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy-rotating-proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "proxy = pd.read_csv(\"Free_Proxy_List.csv\")\n",
    "proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_list = [f\"{row[0]}:{row[1]}\" for row in zip(proxy[\"ip\"],proxy[\"port\"])]\n",
    "proxy_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"proxy.txt\", \"w\")\n",
    "for element in proxy_list:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject rotate_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"rotate_proxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl rotate_proxy -O results/8_rotate_proxy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "[Rotate user agent](https://python.plainenglish.io/rotating-user-agent-with-scrapy-78ca141969fe)\n",
    "\n",
    "[Rotate proxies](https://medium.com/@TeraCrawler.io/how-to-rotate-proxies-in-scrapy-2bccf38439f7)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16196ea7eff63910081d4e10ae1bdb1eb18fd83cb470bb8efbb9fa6b0c724af5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
