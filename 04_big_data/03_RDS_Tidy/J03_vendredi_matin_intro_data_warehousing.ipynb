{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3925fe8-76db-471f-8261-4033bcd297fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Vendredi 05 Avril\n",
    "\n",
    "# Introduction to Data Warehousing\n",
    "\n",
    "The concept of Data Warehousing originated at IBM in the 80's. The goal of the initial research was to provide a framework to transfer data from operational systems to business intelligence departments, avoiding the cost and technical challenges of high redundancy.\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "\n",
    "This lecture will introduce the concept of data warehousing and why do we need it. Here's the outline:\n",
    "\n",
    "* Why analysts cannot work directly on business databases?\n",
    "* Data Warehouse VS Data Lake\n",
    "* Data Warehouse VS traditional databases\n",
    "    * Key differences\n",
    "* Cloud vendors\n",
    "* Amazon Redshift\n",
    "    * Setup your own Redshift cluster\n",
    "    * Tear down your Redshift cluster when you are done\n",
    "* Using Redshift in PySpark\n",
    "    * Writing to Redshift from PySpark DataFrame\n",
    "    * Reading from Redshift onto a PySpark DataFrame\n",
    "\n",
    "## Why analysts cannot work directly on business databases? ü§îü§î\n",
    "\n",
    "Business databases must stay clean at all cost: allowing Data Analysis or Data Scientist to access it introduces a breach.\n",
    "\n",
    "Moreover, most of the time, unstructured data (i.e., not stored in any kind of databases) is required to do performant analysis. \n",
    "\n",
    "A Warehousing solution allows the company to aggregate and store its data needed for analysis, without altering the databases used for operations.\n",
    "\n",
    "## Data Warehouse VS Data Lake üóÑÔ∏èüÜöüåä\n",
    "\n",
    "You often hear both when discussing Big Data, however they are very different.\n",
    "\n",
    "Data Lakes are a big pool of raw data, with no defined purposes: we store this unstructured data in prevision of future usage.\n",
    "\n",
    "Data Warehouse holds **processed** and **structured** data, ready to be used for advanced analytics. \n",
    "\n",
    "Most of the time, data that ends up in the Warehouse was previously stored in the Lake. \n",
    "\n",
    "- Step 1: Data is collected and stored in its raw form in a Data Lake,\n",
    "- Step 2: Data is extracted from the Lake, cleaned and processed,\n",
    "- Step 3: Data is loaded in the warehouse, ready to be queried.\n",
    "\n",
    "## Data Warehouse VS traditional databases üóÑÔ∏èüóÑÔ∏èüóÑÔ∏èüÜöüóÑÔ∏è\n",
    "\n",
    "Roughly, a Data Warehouse **is** a relational database. It's just a little more than that.\n",
    "\n",
    "### Key differences üîë\n",
    "\n",
    "1. The Warehouse can hold data from many databases.\n",
    "2. Any data stored in the Warehouse is stored for **analytics purposes only**.\n",
    "3. Data within a warehouse has been processed to simplify the analysis, and avoid the need for SQL queries that spread on 300 lines.\n",
    "4. Whereas databases are optimized for extracting rows (or observations), data warehouses are optimized to have a performance boost on columns (or fields).\n",
    "\n",
    "In a nutshell: warehouses are optimized for performant analysis.\n",
    "\n",
    "**A warehouse is the perfect candidate for `LOAD` destination in ETL pipelines.**\n",
    "\n",
    "## Cloud vendors ‚òÅÔ∏è‚òÅÔ∏è\n",
    "\n",
    "- BigQuery, owned by Google, and part of the Google Cloud Platform,\n",
    "- Redshift, owned by Amazon and part of the AWS platform,\n",
    "- Snowflake,\n",
    "- ...\n",
    "\n",
    "As always when choosing between different vendors, the cost structure is one the most important aspects to check. For instance, BigQuery storage is **much** cheaper than Redshift, but querying data on Redshift is **free** whereas it costs about 5 dollars/TB on BigQuery. Depending on your need, one solution might be more suitable than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift üî¥üî¥\n",
    "\n",
    "Redshift is the Data Warehousing solution from Amazon Web Services. As every services of the AWS family, Redshift is **Cloud-based**: you only pay for the compute and storage, and you don't have to take care of maintenance costs, or scaling the hardware to support an increasing load.\n",
    "\n",
    "Amazon Redshift Serverless automatically provisions data warehouse capacity and intelligently scales the underlying resources. Amazon Redshift Serverless adjusts capacity in seconds to deliver consistently high performance and simplified operations for even the most demanding and volatile workloads.\n",
    "\n",
    "**If you have never used Amazon Redshift Serverless before, you are eligible for a $300 credit, which can be used within 90 days of sign-up toward your compute and usage use !**\n",
    "\n",
    "### Creating a Redshift Serverless cluster\n",
    "\n",
    "Go to your AWS Console and look for Redshift. Check you are on a good location, here `Paris`. Click on _Try Redshift Serverless free trial_!\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift1.png\"/>\n",
    "\n",
    "In the configuration page, click on \"Manage IAM roles\" > \"Create IAM role\".\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift2.png\"/>\n",
    "\n",
    "Select \"Any S3 bucket\"  and click on \"Create IAM role as default\":\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift3.png\"/>\n",
    "\n",
    "At the end of the page, validate and wait for the cluster initialisation to complete (it may take up to 10 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can click on \"default-workgroup\", and you should read \"Available\" in the status:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift4.png\"/>\n",
    "\n",
    "Scroll down until you see the \"Network and security\" settings, and click on the \"Edit\" button:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift5.png\"/>\n",
    "\n",
    "In the configuration page, check \"Turn on Publicly accessible\":\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift6.png\"/>\n",
    "\n",
    "The changes may take a few minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, your first redshift serverless cluster is ready !\n",
    "\n",
    "Now, let's create a connection between our notebook (on Databricks) and the redshift datawarehouse in order to read/write some data.\n",
    "\n",
    "To do so, you'll need some connection informations:\n",
    "\n",
    "* The JDBC URL is in the \"default-workgroup\" page:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift8.png\"/>\n",
    "\n",
    "* The username is in the \"default-namespace\" page:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshift9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Redshift in PySpark\n",
    "\n",
    "### Writing to Redshift from PySpark DataFrame ‚ú®‚û°üî¥\n",
    "\n",
    "Let's show you how to use Redshift with PySpark. First, we are creating a simple Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dict = {'a': [1,2,3], 'b': [2,3,4], 'c': [3,4,5], 'd':[np.NaN,0,1], 'e':[\"apple\",\"banana\",\"orange\"]}\n",
    "\n",
    "pandas_df = pd.DataFrame.from_dict(\n",
    "    data_dict\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to fill some informations:\n",
    "\n",
    "> The `REDSHIFT_FULL_PATH` is the URL JDBC from the workgroup panel we mentioned above üëÜ. Remember? üôÇ\n",
    "You'll have to modify it, by replacing \"redshift\" by \"postgresql\" in the url.\n",
    "\n",
    "> The `REDSHIFT_USER` is the Admin user name from the namespace panel\n",
    "\n",
    "> The `REDSHIFT_PASSWORD` is the password you set when you created the cluster. \n",
    "If you forgot your password, you can edit it by clicking on the \"Actions\" button in the default-namespace panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFT_USER = 'YOUR_REDSHIFT_USERNAME'\n",
    "REDSHIFT_PASSWORD = 'YOUR_REDSHIFT_PASSWORD'\n",
    "\n",
    "REDSHIFT_FULL_PATH = \"URL_JDBC\" # don't forget to replace \"redshift\" by \"postgresql\"\n",
    "                                # for example it'll look like:\n",
    "                                # \"jdbc:postgresql://redshift-cluster-1.csssws1edn9m.eu-west-3.redshift.amazonaws.com:5439/dev\"\n",
    "REDSHIFT_TABLE = 'NAME_OF_THE_TABLE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write to our Redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "\n",
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df.write.jdbc(url=REDSHIFT_FULL_PATH, table=REDSHIFT_TABLE, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 `mode` to choose from are:\n",
    "\n",
    "- `overwrite`: drop the table if it exists, then load the data in a new one,\n",
    "- `append`: create the table if it does not exists, else append the data to the existing table,\n",
    "- `error` (default): create the table or raise an error if it exists,\n",
    "- `ignore`: same as `overwrite`, but does nothing if table already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've executed the cells above, you can check that the data has been written into redshift, by clicking on the \"query data\" button in the namespace panel. A window like the one below should appear:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshiftquery.png\"/>\n",
    "\n",
    "On the left side, you should find a table in Serverless > dev > public > Tables.\n",
    "You can also use the SQL query editor on the right to query the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Redshift onto a PySpark DataFrame üî¥‚û°‚ú®\n",
    "\n",
    "We can read from our Redshift in few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "table = sqlContext.read.jdbc(url=redshift_path_full, table=REDSHIFT_TABLE, properties=properties)\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this can be useful, it is also possible to query your database using the Redshift query editor directly, which is most likely what data analysts and business analysts would be doing in a real-life context.\n",
    "\n",
    "Congrats! üëè You just created your first data warehouse using Redshift! Do not forget to üëâ **[tear down your Redshift cluster](#how-to-tear-down-your-redshift)** üëà or you run the risk of being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tear down your Redshift?\n",
    "\n",
    "When you have finished working with your Redshift cluster we advise your to ‚ö†Ô∏è **tear down your Redshift cluster so as to avoid too much costs.** ‚ö†Ô∏è\n",
    "\n",
    "It is easy, just follow the following steps:\n",
    "\n",
    "Go to the default-workgroup panel, click on Actions > Delete workgroup\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshiftdelete1.png\"/>\n",
    "\n",
    "In the confirmation page, enter the word \"delete\", then check \"Delete the associated namespace default-namespace\", uncheck \"Create final snapshot\" and write again \"delete\" in the end of the page:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshiftdelete3.png\"/>\n",
    "\n",
    "It may happen that the default-namespace can't be deleted at this point. In this case, some error message will pop-up. Then, just go into the default-namespace panel, click on \"Actions\" > \"Delete namespace\":\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshiftdelete4.png\"/>\n",
    "\n",
    "Uncheck \"create final snapshot\" and write \"delete\" to confirm:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/new/redshiftdelete5.png\"/>\n",
    "\n",
    "You're done ! The deletion is complete if there is no workgroup and no namespace appearing in your redshift dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "- [A nice article on Alooma's blog](https://www.alooma.com/blog/database-vs-data-warehouse)\n",
    "- [Amazon Redshift](https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html#setting-a-custom-column-type)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Introduction_to_Data_Warehousing",
   "notebookOrigID": 4407832410620818,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
