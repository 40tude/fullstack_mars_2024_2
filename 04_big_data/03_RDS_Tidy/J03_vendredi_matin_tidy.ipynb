{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085cdad1-e815-4ecb-8ff4-b7d700e94482",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tidy data and nested schemas\n",
    "\n",
    "Data tidying is the concept of structuring datasets to facilitate analysis.\n",
    "\n",
    "The principles of tidy data have been described in 2013 by statistician [Hadley Wickman](http://hadley.nz/) and closely tied to the principles of relational databases and Codd's relational algebra. They provide a standard way to organize data values within a dataset and can be synthetized as:\n",
    "\n",
    "- Each variable forms a column.\n",
    "- Each observation forms a row.\n",
    "- Each type of observational unit forms a table.\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "This course will demonstrate how to tidy up a Dataframe's schema. Here's the ouline:\n",
    "\n",
    "* Array operations and nested schemas\n",
    "    * `F.size(...)`\n",
    "    * `F.explode(...)`\n",
    "    * `.groupBy()` again ;)\n",
    "    * `.collect_list(...)`\n",
    "* Deep Nested schema\n",
    "    * `.getField(...)`\n",
    "* Even deeper\n",
    "* Advanced groupBy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e1111b-d5f3-427a-9d13-f3cd9113735d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Array operations and nested schemas ‚öôÔ∏è‚öôÔ∏è\n",
    "\n",
    "In this lecture we will introduce some spark sql which we'll need in order to clean our datasets before we run further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbdda82-b14f-4e9f-a768-0c0a01693618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pas utile si on est sur databriks\n",
    "\n",
    "# spark\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc42964d-b910-4e77-9cbe-cbebc6b189d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prelude\n",
    "\n",
    "from pyspark.sql import functions as F # This will load the class where spark sql functions are contained\n",
    "from pyspark.sql import Row # this will let us manipulate rows with spark sql\n",
    "from pyspark.sql.types import * # Import types to convert columns using spark sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b261ed02-473e-4c88-8114-f2ed1cb7b090",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's say we have some data about users, here we create a RDD from a dict, but in real life, we would obtain it through a pipeline or a query from a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5836b431-0ea8-4db6-b60b-41177cd5f3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LEVEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4eb0b7-65dd-45d9-9d7c-b46dcbff339c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n| id|  name|              orders|\n+---+------+--------------------+\n|  1|George|[50.61, 31.32, 20.9]|\n|  2|Hugues|[133.8, 59.0, 40....|\n+---+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "users_dct = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [50.61, 31.32, 20.9]},\n",
    "    {'id': 2, 'name': 'Hugues', 'orders': [133.8, 59.0, 40.03, 27.91]}\n",
    "]\n",
    "\n",
    "users_rdd = sc.parallelize(users_dct)\n",
    "\n",
    "# Les 2 lignes ci-dessous fonctionnent\n",
    "# Sans doute car y a 1 seul niveau\n",
    "# users_df = spark.createDataFrame(users_rdd.map(lambda x: Row(**x))) # this is called unpacking, \n",
    "users_df = spark.createDataFrame(users_rdd)\n",
    "\n",
    "# try this command with Row(x) and Row(*x) to understand what it does\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c06a97-6a73-479e-82dd-01329d2709c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: array (nullable = true)\n |    |-- element: double (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "# The .createDataFrame(...) method is able to infer the data schema by itself\n",
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0de5b8-dcf5-4770-bba3-dce095678626",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Although Spark is able to infer the data schema by itself, it can be useful to design it yourself, let's try and do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abde7e0e-394c-418e-b889-9b658b5da1af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import * # Import types to convert columns using spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3dc0ee-e88c-4ba3-9d42-d469235f73a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n+---+------+-----------------+\n| id|  name|           orders|\n+---+------+-----------------+\n|  1|George|     [50, 31, 20]|\n|  2|Hugues|[133, 59, 40, 27]|\n+---+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "users_dct = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [50, 31, 20]},\n",
    "    {'id': 2, 'name': 'Hugues', 'orders': [133, 59, 40, 27]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users_dct)\n",
    "\n",
    "# we create a variable schema as a list of StructField inside a StructType object\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True), # the first column is of type Integer\n",
    "    StructField('name', StringType(), True), # the second column is a String\n",
    "    StructField('orders', ArrayType(IntegerType()), True) # the third column contains Array of Integer\n",
    "])\n",
    "\n",
    "# Bien voir la lambda\n",
    "#users_df = spark.createDataFrame(users_rdd.map(lambda x: Row(**x)), schema=schema) \n",
    "users_df = spark.createDataFrame(users_rdd, schema=schema)\n",
    "\n",
    "# to the function using the appropriate argument\n",
    "users_df.printSchema()\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906fd42f-4efa-4055-ba79-3b22f84ebd09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### `F.size(...)`\n",
    "\n",
    "This function is able to calculate the number of elements inside an array type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef32439-1d76-45f9-9dc3-35fe347aadf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n| id|  name|orders_quantity|\n+---+------+---------------+\n|  1|George|              3|\n|  2|Hugues|              4|\n+---+------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# On drope orders car pas utile √† l'affichage\n",
    "# Bien voir qu'√† l'affichage, dans orders_quantities, on a le nb d'√©l√©ment de chaque list \n",
    "users_df \\\n",
    "    .withColumn('orders_quantity', F.size('orders'))\\\n",
    "    .drop('orders')                                     \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae84a417-929b-4cf9-8fb3-89cf8ca2e3d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+\n| id|  name|           orders|\n+---+------+-----------------+\n|  1|George|     [50, 31, 20]|\n|  2|Hugues|[133, 59, 40, 27]|\n+---+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Bien garder en t√™te que users_df n'a PAS √©t√© modifi√©\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8da4825-8668-468d-9192-f01eef689e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We get the size of the array, which is pretty nice, but what if we want to compute other aggregates like sum or average? It appears it's not trivial, we will go through one method but there are other, you can read more about it [here](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf34038b-b5ee-436b-9a94-da93197a1cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### `F.explode(...)`\n",
    "Before we try to compute aggregate, let's ask another question: what if we want one row per order?  \n",
    "An order is an observational unit which, according to the tidy principles, deserves it's own table.\n",
    "\n",
    "The explode function will take a column of type array, and make copies of the entire line so that each element of the array be represented on a separate entry of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c36d62-3e7c-48b8-a7c9-1de30fb69f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4801a0-866f-4eb5-9a9d-03cba87c00fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: integer (nullable = true)\n\n+---+------+------+\n| id|  name|orders|\n+---+------+------+\n|  1|George|    50|\n|  1|George|    31|\n|  1|George|    20|\n|  2|Hugues|   133|\n|  2|Hugues|    59|\n|  2|Hugues|    40|\n|  2|Hugues|    27|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# On cr√©√© une \"nouvelle\" colonne orders qui va √©craser la pr√©c√©dente\n",
    "# On \"explose\" le contenu de l'ancienne colonne orders\n",
    "# Pour chaque √©l√©ment de la liste, les lignes se trouvent r√©p√©t√©es \n",
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.printSchema()\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063d7861-7b3f-4c17-8002-570e54030c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### `.goupBy(...)`\n",
    "Now we can compute the average order by customer with a `.groupBy(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fc58b4-31d4-411d-9a75-b4a7fbfd2430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+\n| id|  name|       avg(orders)|\n+---+------+------------------+\n|  1|George|33.666666666666664|\n|  2|Hugues|             64.75|\n+---+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.groupBy('id', 'name') \\\n",
    "    .mean('orders') \\\n",
    "    .show()\n",
    "\n",
    "# here it's ok to just writethe column names, but don't forget that it's usually\n",
    "# better to use the column objects instead to avoid errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fc37c4-8d0b-489f-89c0-420a08dd9c1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### `.collect_list(...)`\n",
    "The opposite transformation is **`.collect_list(...)`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccedca29-5161-4df4-9cdf-82a1c93a490b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+\n| id|  name|           orders|\n+---+------+-----------------+\n|  1|George|     [50, 31, 20]|\n|  2|Hugues|[133, 59, 40, 27]|\n+---+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.groupBy('id', 'name') \\\n",
    "    .agg(F.collect_list('orders').alias('orders')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8fab5c-7948-40ad-8d73-dd37377acc49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We got our original DataFrame back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ddb823-24db-4c39-9e77-7a7e53cf816f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Niveau 2\n",
    "* Y a 2 niveaux d'imbrication\n",
    "* This time our schema will be a bit more difficult, we have a list of users with their orders, but not only we have the order amount, we also some additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8a8475-0efc-4d7d-aab3-3647464f0119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66920b4-7921-45d1-ab63-6a24c76cbe58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: integer (nullable = true)\n |    |    |-- value: float (nullable = true)\n\n+---+------+--------------------+\n| id|  name|              orders|\n+---+------+--------------------+\n|  1|George|[{1, 55.1}, {2, 7...|\n|  2|Hughes|[{3, 31.19}, {5, ...|\n+---+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "users = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [\n",
    "        {'id': 1, 'value': 55.1},\n",
    "        {'id': 2, 'value': 78.31},\n",
    "        {'id': 4, 'value': 52.13}\n",
    "    ]},\n",
    "    {'id': 2, 'name': 'Hughes', 'orders': [\n",
    "        {'id': 3, 'value': 31.19},\n",
    "        {'id': 5, 'value': 131.1}\n",
    "    ]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('orders', ArrayType(\n",
    "        StructType([\n",
    "            StructField('id', IntegerType(), True),\n",
    "            StructField('value', FloatType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "users_df = spark.createDataFrame(users_rdd, schema=schema)\n",
    "users_df.printSchema()\n",
    "users_df.show()\n",
    "\n",
    "# You'll see that the schema this time is a little deeper than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4022fbc9-ab9f-40c1-80f8-b8e45750861e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: struct (nullable = true)\n |    |-- id: integer (nullable = true)\n |    |-- value: float (nullable = true)\n\n+---+------+----------+\n| id|  name|    orders|\n+---+------+----------+\n|  1|George| {1, 55.1}|\n|  1|George|{2, 78.31}|\n|  1|George|{4, 52.13}|\n|  2|Hughes|{3, 31.19}|\n|  2|Hughes|{5, 131.1}|\n+---+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's explode the orders column start unnesting the schema\n",
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.printSchema()\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fea9d4-5b78-46fd-8712-cb104c940d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### `.getField(...)`\n",
    "We can access nested fields using `.getField(fieldname)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffeefe10-362a-4c22-a5f1-4c67fdf84bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+--------+\n| id|  name|    orders|order_id|\n+---+------+----------+--------+\n|  1|George| {1, 55.1}|       1|\n|  1|George|{2, 78.31}|       2|\n|  1|George|{4, 52.13}|       4|\n|  2|Hughes|{3, 31.19}|       3|\n|  2|Hughes|{5, 131.1}|       5|\n+---+------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Se rappeler qu'on avait ce sh√©ma\n",
    "# root\n",
    "#  |-- id: integer (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "#  |-- orders: struct (nullable = true)\n",
    "#  |    |-- id: integer (nullable = true)\n",
    "#  |    |-- value: float (nullable = true)\n",
    "\n",
    "# F.col(\"col_name\") returns the column object just like df.col_name or df[\"col_name\"]\n",
    "\n",
    "# orders_df.printSchema()\n",
    "\n",
    "orders_df \\\n",
    "    .withColumn('order_id', F.col('orders').getField('id')) \\\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9125adda-0f82-47a3-a160-711fcee6bea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pr√©f√©rer la notation `F.col('orders').getField('id')` ?\n",
    "Or using **`.`** notation, just like you would do to access a column inside a DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f2a3e5-19f8-455e-9762-c0e61afbbad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+--------+\n| id|  name|    orders|order_id|\n+---+------+----------+--------+\n|  1|George| {1, 55.1}|       1|\n|  1|George|{2, 78.31}|       2|\n|  1|George|{4, 52.13}|       4|\n|  2|Hughes|{3, 31.19}|       3|\n|  2|Hughes|{5, 131.1}|       5|\n+---+------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df \\\n",
    "    .withColumn('order_id', F.col('orders.id')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a18f0a-00c4-4f25-a199-a2568226c628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----------+\n| id|  name|order_id|order_value|\n+---+------+--------+-----------+\n|  1|George|       1|       55.1|\n|  1|George|       2|      78.31|\n|  1|George|       4|      52.13|\n|  2|Hughes|       3|      31.19|\n|  2|Hughes|       5|      131.1|\n+---+------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's extract both the nested columns to get a flat schema\n",
    "orders_df_flattened = orders_df \\\n",
    "    .withColumn('order_id', F.col('orders').getField('id')) \\\n",
    "    .withColumn('order_value', F.col('orders').getField('value')) \\\n",
    "    .drop('orders')\n",
    "orders_df_flattened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81d9671-b29e-473d-82fa-7e7d13b00086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----------+\n| id|  name|order_id|order_value|\n+---+------+--------+-----------+\n|  1|George|       1|       55.1|\n|  1|George|       2|      78.31|\n|  1|George|       4|      52.13|\n|  2|Hughes|       3|      31.19|\n|  2|Hughes|       5|      131.1|\n+---+------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# M√™me chose avec la notation .\n",
    "orders_df_flattened = orders_df \\\n",
    "    .withColumn('order_id', F.col('orders.id')) \\\n",
    "    .withColumn('order_value', F.col('orders.value')) \\\n",
    "    .drop('orders')\n",
    "orders_df_flattened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3652c6d-355f-4fa0-97e3-9d3d722ccf24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n|  name|  sum(order_value)|\n+------+------------------+\n|Hughes|162.29000663757324|\n|George|185.53999710083008|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# It is now possible to aggregate this table using goupBy and some aggregation function like .sum\n",
    "orders_df_flattened                 \\\n",
    "    .groupBy('name')                \\\n",
    "    .sum('order_value')             \\\n",
    "    .orderBy('sum(order_value)')    \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33677682-476a-41a5-8a52-72c0e4f67375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n|  name|       total_value|\n+------+------------------+\n|George|185.53999710083008|\n|Hughes|162.29000663757324|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aliasing inline and descending sort\n",
    "orders_df_flattened                                 \\\n",
    "    .groupBy('name')                                \\\n",
    "    .agg(F.sum('order_value').alias('total_value')) \\\n",
    "    .orderBy(F.desc('total_value'))                 \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e8f57ff-fc45-4ff4-840b-ad248b0b2850",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Niveau 3\n",
    "Let's now simulate an even deeper nested schema, and we will walk you through the process of unnesting it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cf8b49-250e-40fe-9b53-c1b1769539c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- orders: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: integer (nullable = true)\n |    |    |-- items: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: integer (nullable = true)\n |    |    |    |    |-- category: string (nullable = true)\n |    |    |    |    |-- price: integer (nullable = true)\n |    |    |    |    |-- quantity: integer (nullable = true)\n\n+---+------+--------------------+\n| id|  name|              orders|\n+---+------+--------------------+\n|  1|George|[{1, [{1, shirt, ...|\n|  2|Hughes|[{2, [{4, shorts,...|\n+---+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "users = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [\n",
    "        {'id': 1, 'items': [\n",
    "            {'id': 1, 'category': 'shirt', 'price': 80, 'quantity': 4},\n",
    "            {'id': 2, 'category': 'jeans', 'price': 130, 'quantity': 2}\n",
    "        ]},\n",
    "        {'id': 4, 'items': [\n",
    "            {'id': 1, 'category': 'shirt', 'price': 80, 'quantity': 1},\n",
    "            {'id': 3, 'category': 'shoes', 'price': 240, 'quantity': 1}\n",
    "        ]}\n",
    "    ]},\n",
    "    {'id': 2, 'name': 'Hughes', 'orders': [\n",
    "        {'id': 2, 'items': [\n",
    "            {'id': 4, 'category': 'shorts', 'price': 120, 'quantity': 3},\n",
    "            {'id': 1, 'category': 'shirt', 'price': 180, 'quantity': 2},\n",
    "            {'id': 3, 'category': 'shoes', 'prices': 240, 'quantity': 1}\n",
    "        ]},\n",
    "        {'id': 3, 'items': [\n",
    "            {'id': 5, 'category': 'suit', 'price': 2000, 'quantity': 1}\n",
    "        ]}\n",
    "    ]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('orders', ArrayType(\n",
    "        StructType([\n",
    "            StructField('id', IntegerType(), True),\n",
    "            StructField('items', ArrayType(\n",
    "                StructType([\n",
    "                    StructField('id', IntegerType(), True),\n",
    "                    StructField('category', StringType(), True),\n",
    "                    StructField('price', IntegerType(), True),\n",
    "                    StructField('quantity', IntegerType(), True)\n",
    "                ])\n",
    "            ))\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "users_df = spark.createDataFrame(users_rdd, schema=schema)\n",
    "users_df.printSchema()\n",
    "users_df.show()\n",
    "\n",
    "# This schema is much deeper than the other two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191221c2-451f-4777-9f15-1263035cdf67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n| id|  name|              orders|\n+---+------+--------------------+\n|  1|George|{1, [{1, shirt, 8...|\n|  1|George|{4, [{1, shirt, 8...|\n|  2|Hughes|{2, [{4, shorts, ...|\n|  2|Hughes|{3, [{5, suit, 20...|\n+---+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# We start by exploding the orders column, which where the nest resides\n",
    "# Bien voir le ORDERS_DF car on descend d'un niveau\n",
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ff1c43-4bd7-4909-8025-04a98610c9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now brace yourselves as we will walk you step by step through the process of unnesting this data schema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfa9a0c-fb47-444c-b5e7-85d0ab97ccaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Etape interm√©diaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f902ebb9-17df-4f6c-bb42-8338f980e7db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9d4887-c109-4dcb-bffd-78b42965529b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+--------------------+\n| id|  name|order_id|               items|\n+---+------+--------+--------------------+\n|  1|George|       1|[{1, shirt, 80, 4...|\n|  1|George|       4|[{1, shirt, 80, 1...|\n|  2|Hughes|       2|[{4, shorts, 120,...|\n|  2|Hughes|       3|[{5, suit, 2000, 1}]|\n+---+------+--------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df = orders_df.withColumn('order_id', F.col('orders.id')) \\\n",
    "    .withColumn('items', F.col('orders.items')) \\\n",
    "    .drop('orders') \\\n",
    "    .show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45fe05ca-b626-49b7-a585-e15953733e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2\n",
    "* **Pour lancer  le code ci-dessous...** il ne faut **PAS** avoir lanc√© les lignes pr√©c√©dentes \n",
    "* Ou alors il faut relancer les lignes o√π on cr√©e le ``orders_df``\n",
    "* Voir `orders_df = users_df.withColumn('orders', F.explode('orders'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003176b2-07fd-4a87-bc85-317fff9778a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-------+--------+-----+--------+\n| id|  name|order_id|item_id|category|price|quantity|\n+---+------+--------+-------+--------+-----+--------+\n|  1|George|       1|      1|   shirt|   80|       4|\n|  1|George|       1|      2|   jeans|  130|       2|\n|  1|George|       4|      1|   shirt|   80|       1|\n|  1|George|       4|      3|   shoes|  240|       1|\n|  2|Hughes|       2|      4|  shorts|  120|       3|\n|  2|Hughes|       2|      1|   shirt|  180|       2|\n|  2|Hughes|       2|      3|   shoes| null|       1|\n|  2|Hughes|       3|      5|    suit| 2000|       1|\n+---+------+--------+-------+--------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.withColumn('order_id', F.col('orders.id')) \\\n",
    "    .withColumn('items', F.col('orders.items')) \\\n",
    "    .drop('orders') \\\n",
    "    .withColumn('item', F.explode('items')) \\\n",
    "    .drop('items') \\\n",
    "    .withColumn('item_id', F.col('item').getField('id')) \\\n",
    "    .withColumn('category', F.col('item').getField('category')) \\\n",
    "    .withColumn('price', F.col('item').getField('price')) \\\n",
    "    .withColumn('quantity', F.col('item').getField('quantity')) \\\n",
    "    .drop('item') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3594874-516d-430c-9c87-de55ae7e7e30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n|user_id|user_name|order_id|item_id|item_category|item_price|item_quantity|total_price|\n+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n|      1|   George|       1|      1|        shirt|        80|            4|        320|\n|      1|   George|       1|      2|        jeans|       130|            2|        260|\n|      1|   George|       4|      1|        shirt|        80|            1|         80|\n|      1|   George|       4|      3|        shoes|       240|            1|        240|\n|      2|   Hughes|       2|      4|       shorts|       120|            3|        360|\n|      2|   Hughes|       2|      1|        shirt|       180|            2|        360|\n|      2|   Hughes|       2|      3|        shoes|      null|            1|       null|\n|      2|   Hughes|       3|      5|         suit|      2000|            1|       2000|\n+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "items_df = (\n",
    "    orders_df.withColumn('order_id', F.col('orders').getField('id'))\n",
    "    .withColumn('items', F.col('orders').getField('items'))\n",
    "    .drop('orders')\n",
    "    .withColumnRenamed('name', 'user_name')\n",
    "    .withColumnRenamed('id', 'user_id')\n",
    "    .withColumn('items', F.explode('items'))\n",
    "    .withColumn('item_id', F.col('items').getField('id'))\n",
    "    .withColumn('item_category', F.col('items').getField('category'))\n",
    "    .withColumn('item_price', F.col('items').getField('price'))\n",
    "    .withColumn('item_quantity', F.col('items').getField('quantity'))\n",
    "    .withColumn('total_price', F.col('item_price') * F.col('item_quantity'))\n",
    "    .drop('items')\n",
    ")\n",
    "items_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0cb40f8-cfcf-4873-874f-5a3972f5dd26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "M√™me chose en notation dot `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426257ee-db6f-4504-8100-c32b19adc9c7",
     "showTitle": false,
     "title": ""
    },
    "code_folding": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n|user_id|user_name|order_id|item_id|item_category|item_price|item_quantity|total_price|\n+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n|      1|   George|       1|      1|        shirt|        80|            4|        320|\n|      1|   George|       1|      2|        jeans|       130|            2|        260|\n|      1|   George|       4|      1|        shirt|        80|            1|         80|\n|      1|   George|       4|      3|        shoes|       240|            1|        240|\n|      2|   Hughes|       2|      4|       shorts|       120|            3|        360|\n|      2|   Hughes|       2|      1|        shirt|       180|            2|        360|\n|      2|   Hughes|       2|      3|        shoes|      null|            1|       null|\n|      2|   Hughes|       3|      5|         suit|      2000|            1|       2000|\n+-------+---------+--------+-------+-------------+----------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "items_df = (\n",
    "    orders_df.withColumn('order_id', F.col('orders.id'))\n",
    "    .withColumn('items', F.col('orders.items'))\n",
    "    .drop('orders')\n",
    "    .withColumnRenamed('name', 'user_name')\n",
    "    .withColumnRenamed('id', 'user_id')\n",
    "    .withColumn('items', F.explode('items'))\n",
    "    .withColumn('item_id', F.col('items.id'))\n",
    "    .withColumn('item_category', F.col('items.category'))\n",
    "    .withColumn('item_price', F.col('items.price'))\n",
    "    .withColumn('item_quantity', F.col('items.quantity'))\n",
    "    .withColumn('total_price', F.col('item_price') * F.col('item_quantity'))\n",
    "    .drop('items')\n",
    ")\n",
    "items_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fabb3d2-5e6c-4aa4-95a1-001f3e57e5fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is much better.\n",
    "Unnesting may be a tedious task but it is an essential part of the process towards facilitating analysis, running analysis and sql type queries on a nested schema is hard, so it is definitely worthspending some time preparing your data so that everyone else saves time when they query your tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54380a0-c020-45d9-9fd8-7fb9d31f9fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Advanced groupBy üßÆüßÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9c8743-c85e-463e-9271-d6a30a34c753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------+------------------+\n",
       "item_category|sum(item_quantity)|\n",
       "+-------------+------------------+\n",
       "        shirt|                 7|\n",
       "       shorts|                 3|\n",
       "        shoes|                 2|\n",
       "        jeans|                 2|\n",
       "         suit|                 1|\n",
       "+-------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------------+------------------+\n|item_category|sum(item_quantity)|\n+-------------+------------------+\n|        shirt|                 7|\n|       shorts|                 3|\n|        shoes|                 2|\n|        jeans|                 2|\n|         suit|                 1|\n+-------------+------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we group the data by item category and calculate the sum\n",
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .sum('item_quantity') \\\n",
    "    .orderBy(F.desc('sum(item_quantity)')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bf1770b-2ca7-4652-86f2-36f442ffde5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You might want to alias, in this case, you change `.sum()` for `.agg()`. This is a little beyond the scope of today's lecture, but we'll show it to you before spending more time understanding aggregates in the following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c853569-bf09-4cc1-a0d8-1408bdfe612d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------+--------------+\n",
       "item_category|total_quantity|\n",
       "+-------------+--------------+\n",
       "        shirt|             7|\n",
       "       shorts|             3|\n",
       "        shoes|             2|\n",
       "        jeans|             2|\n",
       "         suit|             1|\n",
       "+-------------+--------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------------+--------------+\n|item_category|total_quantity|\n+-------------+--------------+\n|        shirt|             7|\n|       shorts|             3|\n|        shoes|             2|\n|        jeans|             2|\n|         suit|             1|\n+-------------+--------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .agg(F.sum('item_quantity').alias('total_quantity')) \\\n",
    "    .orderBy(F.desc('total_quantity')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e74b7a5-8568-4e2d-8d06-7e752c1aea92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If I want to alias.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9250ff90-5c5d-414d-b3e1-21b3615ecf75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------+------------------+\n",
       "item_category|          avg_sale|\n",
       "+-------------+------------------+\n",
       "         suit|            2000.0|\n",
       "        jeans|             130.0|\n",
       "        shoes|             120.0|\n",
       "       shorts|             120.0|\n",
       "        shirt|108.57142857142857|\n",
       "+-------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------------+------------------+\n|item_category|          avg_sale|\n+-------------+------------------+\n|         suit|            2000.0|\n|        jeans|             130.0|\n|        shoes|             120.0|\n|       shorts|             120.0|\n|        shirt|108.57142857142857|\n+-------------+------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .agg((F.sum('total_price') / F.sum('item_quantity')).alias('avg_sale')) \\\n",
    "    .orderBy(F.desc('avg_sale')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ca8a78-b69b-4a7c-98e4-95973e07a887",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* We strongly advice you take the time to read [the original paper from Wickam](https://vita.had.co.nz/papers/tidy-data.pdf).\n",
    "You might want to look at this ressource which will be used in the exercises in order to flatten a very nested data schema.\n",
    "* [Automatically and Elegantly flatten DataFrame in Spark SQL](https://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql) on StackOverflow"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "J03_vendredi_matin_tidy",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
