{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f1398c-40f8-41d0-ac5d-5046c3e487a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Words count with PySpark RDDs\n",
    "If you've ever heard of \"Hello, world!\" for web development, \"Word count\" is the actual equivalent for distributed computing.\n",
    "\n",
    "In this notebook, we will setup a pipeline that will count the words in a document in a distributed manner. For convenience, we will do this on a single small document, but the operation should easily generalize to bigger documents that would not fit into the memory of a single machine. And that's the idea of this whole module, we work with technology that is able to scale according to the task at hand, even if we practice with smaller data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ae3d88-b600-4878-8e86-7ac121d4ee26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We start by defining the spark context to play with RDDs\n",
    "# Pas besoin si on est sur databricks\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de50764a-6e2a-4dbc-a266-9c5194aa21fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need a S3 filepath\n",
    "\n",
    "FILENAME = 's3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e88eb7f-d070-43f1-bd7e-99ec637e0b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Load the filepath to a Spark RDD using `.textFile(...)` from a SparkContext into `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ced4916-b485-4875-8735-db2b8b6a6820",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff8da73-64c8-4e8a-b7ca-f3307b0a0359",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Print out `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb3d0db-1440-4a9a-af02-d556394a8c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['I never meant to cause you any sorrow',\n",
      " 'I never meant to cause you any pain',\n",
      " 'I only wanted one time to see you laughing',\n",
      " 'I only want to see you laughing in the purple rain',\n",
      " '',\n",
      " 'Purple rain, purple rain',\n",
      " 'Purple rain, purple rain',\n",
      " 'Purple rain, purple rain',\n",
      " '',\n",
      " 'I only want to see you bathing in the purple rain',\n",
      " '',\n",
      " 'I never wanted to be your weekend lover',\n",
      " 'I only wanted to be some kind of friend',\n",
      " 'Baby, I could never steal you from another',\n",
      " \"It's such a shame our friendship had to end\",\n",
      " '',\n",
      " 'Purple rain, purple rain',\n",
      " 'Purple rain, purple rain',\n",
      " 'Purple rain, purple rain',\n",
      " '',\n",
      " 'I only want to see you underneath the purple rain',\n",
      " '',\n",
      " 'Honey I know, I know, I know times are changing',\n",
      " \"It's time we all reach out for something new\",\n",
      " 'That means you too',\n",
      " 'You say you want a leader',\n",
      " \"But you can't seem to make up your mind\",\n",
      " 'I think you better close it',\n",
      " 'And let me guide you to the purple rain',\n",
      " '',\n",
      " 'Purple rain, purple rain',\n",
      " 'Purple rain, purple rain',\n",
      " '',\n",
      " \"If you know what I'm singing about up here\",\n",
      " \"C'mon, raise your hand\",\n",
      " '',\n",
      " 'Purple rain, purple rain',\n",
      " '',\n",
      " 'I only want to see you, only want to see you',\n",
      " 'In the purple rain']"
     ]
    }
   ],
   "source": [
    "text_file.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c8e5680-2400-4e4c-aead-c770c530cb61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. That doesn't tell us much, how would you do to see the first 3 elements of this RDD? take the first 3 elements of the RDD `text_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd9a55c-0857-4740-8e64-772337e691b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['I never meant to cause you any sorrow',\n",
      " 'I never meant to cause you any pain',\n",
      " 'I only wanted one time to see you laughing']"
     ]
    }
   ],
   "source": [
    "text_file.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b6f7b8-e656-4308-bd27-867a5e89cbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. This is a list of sentences, what we want is a list of tokens. Use the map function and a string method in order to split the charater strings into lists of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42b030d-e2de-4f7c-9e8f-2891ca510bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [['I', 'never', 'meant', 'to', 'cause', 'you', 'any', 'sorrow'],\n",
      " ['I', 'never', 'meant', 'to', 'cause', 'you', 'any', 'pain'],\n",
      " ['I', 'only', 'wanted', 'one', 'time', 'to', 'see', 'you', 'laughing']]"
     ]
    }
   ],
   "source": [
    "tokens_list = text_file.map(lambda s: s.split(\" \"))\n",
    "tokens_list.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748172f9-85fa-409c-8fc6-27facc2c0779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's not exactly what we wanted... We wanted a list of tokens, we got a... **list of list of tokens**!\n",
    "\n",
    "That's because, in this case, we need a special version of `.map()` called `flatMap`: it will flatten the list of list of tokens into a list of tokens.\n",
    "\n",
    "Let's try it out: we take the same expression as the previous one, but replace `.map()` with `.flatMap()` and call the resulting variable `tokens`.\n",
    "\n",
    "---\n",
    "💡 It usually takes time to understand the notion of `.flatMap` and flattening in general, like `.map()`, these are concepts from the functionnal programming world. Unless you come from such a background, it probably **won't be easy to grasp these concepts the first time you encouter them**.\n",
    "\n",
    "**Let's keep our eyes on the goal: to develop a broader understanding of how Spark works.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f1638df-5e82-41dd-b1c4-c1e2ae1af3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. Copy/paste the previous cell, and:\n",
    "- replace `.map(...)` with `.flatMap(...)`\n",
    "- rename the variable `tokenized_text` to `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752b4838-81c5-4a54-a898-a619d3c383bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: ['I', 'never', 'meant']"
     ]
    }
   ],
   "source": [
    "tokens = text_file.flatMap(lambda s: s.split(\" \"))\n",
    "tokens.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5545913-42df-448b-a04b-f0ace851d2f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. Use this cell to play with `tokens`, take different amounts of it, or collect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bd70bc-24f1-438e-baf0-e5e79d7567d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ['I', 'never', 'meant', 'to', 'cause', 'you', 'any', 'sorrow', 'I', 'never']"
     ]
    }
   ],
   "source": [
    "tokens.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e2bc2e3-62bc-46e5-a775-2ad14d1f00af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have our list of words (well, **not exactly a list of words, it is still a RDD**), we can start counting things.\n",
    "\n",
    "In order to do that, we need to map each word to an initial count, so instead of having:\n",
    "```\n",
    "['I',\n",
    " 'never',\n",
    " 'meant',\n",
    " ...,\n",
    " 'I',\n",
    " 'never',\n",
    " ...]\n",
    "```\n",
    "We would like our list to look like this:\n",
    "```\n",
    "[('I', 1),\n",
    " ('never', 1),\n",
    " ('meant', 1),\n",
    " ...,\n",
    " ('I', 1),\n",
    " ('never', 1),\n",
    " ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0298e267-2656-4821-b383-68f6f8d52785",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. Write a function `token_to_tuple` that takes:\n",
    "- a token as input (a string)\n",
    "- and returns (token, 1) (a tuple) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad38100e-8267-47df-9e6c-5523f6f3d1df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def token_to_tuple(token):\n",
    "  return (token, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec62eb7-c786-42b8-8e5f-08525759bdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. map `tokens` to your new function `token_to_tuple` to create a variable called `partial_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92894ab6-39cd-4b9f-a94c-afeda6c74d08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# partial_count = tokens.map(lambda t : token_to_tuple(t))\n",
    "partial_count = tokens.map(token_to_tuple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc43c71-6358-4ebf-9816-6cf91404d237",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. Take the first 10 elements of `partial_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82738211-77ff-45b3-a070-cd9b67e98a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[77]: [('I', 1),\n",
      " ('never', 1),\n",
      " ('meant', 1),\n",
      " ('to', 1),\n",
      " ('cause', 1),\n",
      " ('you', 1),\n",
      " ('any', 1),\n",
      " ('sorrow', 1),\n",
      " ('I', 1),\n",
      " ('never', 1)]"
     ]
    }
   ],
   "source": [
    "partial_count.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9247d56-82c0-49d9-be4b-b46e524756d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0761bfa5-7161-44b0-ab54-ea65a4882cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Beware, now comes the hard-part... We need to reduce this.\n",
    "\n",
    "Don't forget RDD are very low level objects, when we start using DataFrames (the other main kind of data format) all of this will become easier because of their higher level of abstraction.\n",
    "\n",
    "What we want is to take tuples with the same key, like `('never', 1)` and `('never', 1)` and aggregate them, so in the end we have `('never', 2)` (or more than 2 if there are more occurences of 'never').\n",
    "\n",
    "These kind of tuples are called **key-value pairs**, and while most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. You can read more about it [in the documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#working-with-key-value-pairs).\n",
    "\n",
    "Among these operations are `.groupByKey(...)` and `.reduceByKey(...)`: the latter has better performances, but the former is easier to understand so we will start with this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f1395a-b2ac-418c-b40f-3cd2a9fb503f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c78ea26d-9ec7-4aeb-8125-713fef4a8fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "10. Call `.groupByKey(...)` on `partial_count` and put it inside `grouped_by_key` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc79947-091d-4d6a-8ea1-4ddac6a59ca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_by_key = partial_count.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3c2fb1-f605-4763-a983-d8f009561bf4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "11.  take the first 3 elements of `grouped_by_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21afd44-ee0a-48a8-b6f4-348dd50e4ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [('never', <pyspark.resultiterable.ResultIterable at 0x7f3d0ed86e20>),\n",
      " ('cause', <pyspark.resultiterable.ResultIterable at 0x7f3d0ed86310>),\n",
      " ('pain', <pyspark.resultiterable.ResultIterable at 0x7f3d1595a790>)]"
     ]
    }
   ],
   "source": [
    "grouped_by_key.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a333cde-a12f-41a2-96e0-2a3ba8b7f978",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What's this: `<pyspark.resultiterable.ResultIterable at 0x10bc0c2d0>` ?\n",
    "\n",
    "You don't have to worry about the details, but one thing has to attract your attention: `Iterable`, this seems to suggest those objects are iterable, an iterable in Python is something that you can iterate on: basically something that you can call `for` on, like a list, or a string, etc.\n",
    "\n",
    "```python\n",
    "for letter in 'Spark':\n",
    "    print(e)\n",
    "> S\n",
    "> p\n",
    "> a\n",
    "> r\n",
    "> k\n",
    "```\n",
    "\n",
    "Each element of `grouped_by_key` is a tuple, and inside a tuple there is an iterable we can iterate over.\n",
    "\n",
    "We will first try with the first element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ea24fb-31cd-481f-a6fe-6eaeb58151fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "12. Take the first element of `grouped_by_key`, put it in `first_item` variable and print out its type.\n",
    "\n",
    "_WARNING: the type should be a tuple, not a list._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78646ec9-dfe6-404c-8c70-015da6c81724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[79]: ('never', <pyspark.resultiterable.ResultIterable at 0x7f3d0ec10f10>)"
     ]
    }
   ],
   "source": [
    "grouped_by_key.collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c44d4db-acab-4674-b326-e0a40a2f27b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[78]: ('never', <pyspark.resultiterable.ResultIterable at 0x7f3d0ec08700>)"
     ]
    }
   ],
   "source": [
    "grouped_by_key.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f175b4-d5b3-4011-af65-b344633f6f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'd like a way to print these items, for example, such that 'never' would look like this:\n",
    "```\n",
    "'never': [1, 1, 1, 1]\n",
    "```\n",
    "\n",
    "We will write a function that does this, take an item as a tuple of (`str`, `ResultIterable`), and print out:\n",
    "```\n",
    "ITEM_NAME: OCCURENCES_AS_A_LIST\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bee20b4-4373-427b-9b28-0b08dd53fe81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "13. Define a way to print our items like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e98f773-f170-42c1-b08a-466650790c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_items(str, resultIterable):\n",
    "  # print(str, *resultIterable)\n",
    "  print(f\"{str} : {[i for i in resultIterable]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f8c007-a614-4bd6-aa22-bf10b562a076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never : [1, 1, 1, 1]\n",
      "cause : [1, 1]\n",
      "pain : [1]\n",
      "only : [1, 1, 1, 1, 1, 1, 1]\n",
      "in : [1, 1]\n",
      "rain : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Purple : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "rain, : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "bathing : [1]\n"
     ]
    }
   ],
   "source": [
    "for str, lst in grouped_by_key.take(10): \n",
    "  print_items(str, lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c49773-c40f-4fde-be7f-d807a7ce90a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_items2(str, resultIterable):\n",
    "  my_list = list(resultIterable)\n",
    "  print(f\"{str} : {my_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e006146-cebf-497c-90af-8eb189fa2e47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never : [1, 1, 1, 1]\n",
      "cause : [1, 1]\n",
      "pain : [1]\n",
      "only : [1, 1, 1, 1, 1, 1, 1]\n",
      "in : [1, 1]\n",
      "rain : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Purple : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "rain, : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "bathing : [1]\n"
     ]
    }
   ],
   "source": [
    "for str, lst in grouped_by_key.take(10): \n",
    "  print_items2(str, lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2995d06-f52c-4eaa-8a65-8689a2490266",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "14. take the first 10 items from `grouped_by_key` and then iterate over them. Then, inside the loop, use the function `print_item(...)` on each item:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2485ee18-7276-4f19-a02d-d1df5fc79861",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next step might be challenging.\n",
    "\n",
    "When you take the first 10 elements of `grouped_by_key`, it returns a list of `Tuple[str, ResultIterable]`.  \n",
    "What we want instead is a list of `Tuple[str, int]` where the second element is the total number of occurence for the fist element.\n",
    "\n",
    "NOTE: you might wanna try to first return a list of `Tuple[str, list]`.\n",
    "\n",
    "You should be able to do all this using only list comprehensions.\n",
    "\n",
    "15. Follow previous instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0df028b-b59c-4ce5-8599-d81ae4117194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never : 4\n",
      "cause : 2\n",
      "pain : 1\n",
      "only : 7\n",
      "in : 2\n",
      "rain : 14\n",
      " : 10\n",
      "Purple : 9\n",
      "rain, : 9\n",
      "bathing : 1\n"
     ]
    }
   ],
   "source": [
    "def print_items2(str, resultIterable):\n",
    "  print(f\"{str} : {sum(resultIterable)}\")\n",
    "\n",
    "for str, lst in grouped_by_key.take(10): \n",
    "  print_items2(str, lst)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a154155-7f12-4554-9180-ddf611baf0ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you've seen this can be done using standard list comprehension.  \n",
    "If you're curious, even though Python is not a purely functional language, you can write this in a functional fashion and achieve the same result.\n",
    "\n",
    "_Please note this would look obviouly more elegant in a purely functional language.  \n",
    "And I put it in there only to introduce you to another programming paradigm if you've mostly encountered imperative programming before. This little introduction is helpful because Spark is based on Scala, which, although not being a purely functional language, provides support for many functional programming features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ac4e38-3c0d-4691-9e8e-bf3f10ee25a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[65]: [('never', 4),\n",
      " ('cause', 2),\n",
      " ('pain', 1),\n",
      " ('only', 7),\n",
      " ('in', 2),\n",
      " ('rain', 14),\n",
      " ('', 10),\n",
      " ('Purple', 9),\n",
      " ('rain,', 9),\n",
      " ('bathing', 1)]"
     ]
    }
   ],
   "source": [
    "# Don't try at home ;)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "list(map(lambda t: (t[0], reduce(lambda a, b: a + b, t[1])),\n",
    "         grouped_by_key.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cdfefc-b9bf-4dff-bc17-3b877629a221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That would work, but that's using regular Python, hence we're not profiting from Spark's distributed computing capabilities, which means:\n",
    "\n",
    "- the computation would be much slower on big datasets,\n",
    "- if the datasets is too big to be stored on the memory of our machine our program would crash.\n",
    "\n",
    "That's exactly what `.reduceByKey(...)` will help us to solve. It's usage is a bit similar to `.groupByKey(...)` but it takes a function as a parameter, this function should tell Spark how to aggregate 2 items, in our case, that the value of each tuple. For example, let's say we have a group:\n",
    "\n",
    "```python\n",
    "('dog', 1), ('dog', 1)\n",
    "```\n",
    "\n",
    "We want a formula applied on values that will give us the end result, e.g. \"how many dogs\". In our case, that's a simple sum:\n",
    "\n",
    "```python\n",
    "def reduce_function(value_1, value_2):\n",
    "    return value_1 + value_2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff0d7a7-0e33-4f28-9c53-bece8a7b3ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "16. Write our reduce function: reduce_function which takes 2 values and return their sum:\n",
    "\n",
    "_NOTE: name these parameters `a` and `b`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec34b385-ac37-4d7e-b7c6-b744033a7618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reduce_function(v1, v2):\n",
    "    return v1 + v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264306c5-874d-4864-94c1-99fabe4d7f94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "17. We're now ready to reduce. You will pass your function as parameter to `.reduceByKey(...)`. call `.reduceByKey(...)` on `partial_count`, name it `reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "493fa0d2-6815-42dd-9ff3-65086c36e1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reduced = partial_count.reduceByKey(reduce_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210c85e1-1fd9-48c3-a395-93ecc2f7d5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "18. Take the 10 first values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30039e8-6763-4a22-a37e-8cbda56b16a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[86]: [('never', 4),\n",
      " ('cause', 2),\n",
      " ('pain', 1),\n",
      " ('only', 7),\n",
      " ('in', 2),\n",
      " ('rain', 14),\n",
      " ('', 10),\n",
      " ('Purple', 9),\n",
      " ('rain,', 9),\n",
      " ('bathing', 1)]"
     ]
    }
   ],
   "source": [
    "reduced.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff7f41c8-2bb3-4325-b21f-f0e646ff626b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We're almost there... 😅\n",
    "We've got a list of tuples, where the key is the token, and the value is its count within the text, but.. \n",
    "**they're not ordered...** which is inconvenient if we want to have the 10 most popular tokens within the text.\n",
    "\n",
    "We will use `.sortBy(...)`, but before we do, let's have a refresher on sorting with Python.\n",
    "\n",
    "For example, how would you sort this grocery list by the number of items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089be583-dd9a-42a5-b7c0-2b1991b88cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[72]: [('banana', 3), ('orange', 5), ('pineapple', 2)]"
     ]
    }
   ],
   "source": [
    "fruits = [('banana', 3), ('orange', 5), ('pineapple', 2)]\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b20b92-f8cf-4c57-8069-edfc732c34c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`sorted(fruits)` won't work because by default sorting on tuple take the first element, in our case, it would sort alphabetically on the name of the fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a37670-b88a-48e8-a290-76dd9999a4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[73]: [('banana', 3), ('orange', 5), ('pineapple', 2)]"
     ]
    }
   ],
   "source": [
    "sorted(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca891d4-339f-402a-96a8-05b2574859b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can force the `key` parameter to sort on the second item of each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8df4918-c1b3-4150-85e5-a08ebf7b254d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[75]: [('pineapple', 2), ('banana', 3), ('orange', 5)]"
     ]
    }
   ],
   "source": [
    "sorted(fruits, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e70edec-d923-4c00-9f68-5c042af52189",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "19. Now, we will do the same on our rdd. Just like `key` in Python's `sorted`, PySpark's `.sortBy(...)` can take a function as a parameter. Use `.sortBy(...)` on `reduced`, name it `sorted_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4b7de4-450b-40a9-962e-fd0509ee60bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted_counts = sorted(reduced.collect(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c018b2-6ec9-465c-aeb7-387d7db06e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "20. Take the 10 first values of `sorted_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e4ba55-6a70-410c-8294-31943b23cde1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[93]: [('pain', 1),\n",
      " ('bathing', 1),\n",
      " ('kind', 1),\n",
      " ('of', 1),\n",
      " ('steal', 1),\n",
      " ('end', 1),\n",
      " ('underneath', 1),\n",
      " ('are', 1),\n",
      " ('we', 1),\n",
      " ('out', 1),\n",
      " ('something', 1),\n",
      " ('new', 1),\n",
      " ('That', 1),\n",
      " ('means', 1),\n",
      " ('say', 1),\n",
      " ('leader', 1),\n",
      " ('But', 1),\n",
      " ('make', 1),\n",
      " ('mind', 1),\n",
      " ('think', 1),\n",
      " ('close', 1),\n",
      " ('And', 1),\n",
      " ('let', 1),\n",
      " ('guide', 1),\n",
      " ('singing', 1),\n",
      " (\"C'mon,\", 1),\n",
      " ('sorrow', 1),\n",
      " ('one', 1),\n",
      " ('weekend', 1),\n",
      " ('lover', 1),\n",
      " ('some', 1),\n",
      " ('friend', 1),\n",
      " ('Baby,', 1),\n",
      " ('could', 1),\n",
      " ('from', 1),\n",
      " ('another', 1),\n",
      " ('such', 1),\n",
      " ('shame', 1),\n",
      " ('our', 1),\n",
      " ('friendship', 1),\n",
      " ('had', 1),\n",
      " ('Honey', 1),\n",
      " ('times', 1),\n",
      " ('changing', 1),\n",
      " ('all', 1),\n",
      " ('reach', 1),\n",
      " ('for', 1),\n",
      " ('too', 1),\n",
      " ('You', 1),\n",
      " (\"can't\", 1),\n",
      " ('seem', 1),\n",
      " ('better', 1),\n",
      " ('it', 1),\n",
      " ('me', 1),\n",
      " ('If', 1),\n",
      " ('what', 1),\n",
      " (\"I'm\", 1),\n",
      " ('about', 1),\n",
      " ('here', 1),\n",
      " ('raise', 1),\n",
      " ('hand', 1),\n",
      " ('you,', 1),\n",
      " ('In', 1),\n",
      " ('cause', 2),\n",
      " ('in', 2),\n",
      " (\"It's\", 2),\n",
      " ('know', 2),\n",
      " ('meant', 2),\n",
      " ('any', 2),\n",
      " ('time', 2),\n",
      " ('laughing', 2),\n",
      " ('be', 2),\n",
      " ('a', 2),\n",
      " ('know,', 2),\n",
      " ('up', 2),\n",
      " ('wanted', 3),\n",
      " ('your', 3),\n",
      " ('never', 4),\n",
      " ('the', 5),\n",
      " ('see', 6),\n",
      " ('want', 6),\n",
      " ('only', 7),\n",
      " ('Purple', 9),\n",
      " ('rain,', 9),\n",
      " ('', 10),\n",
      " ('to', 13),\n",
      " ('rain', 14),\n",
      " ('I', 14),\n",
      " ('you', 14),\n",
      " ('purple', 14)]"
     ]
    }
   ],
   "source": [
    "sorted_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b3444b-3d6d-483f-983a-01761256f005",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a60fd53-1f19-4e47-98a2-8e3cb5698f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It seems sorted, but in **ascending order**!\n",
    "\n",
    "If we wanted to do this in Python, we could just set the `reverse` argument to `True` when calling `sorted(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b60a2faa-8ff4-47b6-9d43-b07e9937cf15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[31]: [(&#39;orange&#39;, 5), (&#39;banana&#39;, 3), (&#39;pineapple&#39;, 2)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[31]: [(&#39;orange&#39;, 5), (&#39;banana&#39;, 3), (&#39;pineapple&#39;, 2)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(fruits, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1548ef8e-e5e7-4a9d-8dbb-f060dd761473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can't do this with Spark's RDDs. What we can do instead is **take the opposite value and order by it**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddbb5663-275a-4e76-9881-acd70f17ace4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "21. Use `.sortBy(...)` on `reduced`, but with a descending sort, name it `desc_sorted_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0cd2f44-bd74-40b3-b79c-aad207914a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desc_sorted_counts = reduced.sortBy(lambda t : -t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac0436b2-091b-407c-91c6-15d0b891d883",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "22. Take the 10 first values of `desc_sorted_counts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abbbdd2-4993-4633-a5d7-cbf1595bdfb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[97]: [('rain', 14),\n",
      " ('I', 14),\n",
      " ('you', 14),\n",
      " ('purple', 14),\n",
      " ('to', 13),\n",
      " ('', 10),\n",
      " ('Purple', 9),\n",
      " ('rain,', 9),\n",
      " ('only', 7),\n",
      " ('see', 6)]"
     ]
    }
   ],
   "source": [
    "desc_sorted_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdbeeb48-79ab-40f0-b789-4139f5c7e66c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, what's the most common word in our document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3fa5da0-79ee-4988-9753-466466f2c888",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Bonus**: putting everything together\n",
    "\n",
    "We will create a function `count_words` that will do everything we did previously, but this time in one swell swoop, we won't use intermediary variables.\n",
    "\n",
    "The function will:\n",
    "- take a filepath as argument\n",
    "- load the content of this filepath into a Spark RDD\n",
    "- `flatMap(...)` each line of this RDD into tokens by splitting on the ' ' string\n",
    "- `.map(...)` each token to `(token, 1)` so this can be then reduced\n",
    "- by calling `.reduceByKey(...)` with a function that sums the values\n",
    "- and then sort the results with `.sortBy(...)` using the proper function to sort in descending order\n",
    "- and return an RDD\n",
    "\n",
    "---\n",
    "⚠️ Make sure your function returns a RDD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45add4da-07f1-40b0-bec7-77113ddc5d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_words(filepath):\n",
    "    # TODO: implement the content of the function\n",
    "    # \n",
    "    # NOTE: you can remove `pass`\n",
    "    # it's just here to avoid the cell crashing while the\n",
    "    # content of the function is empty\n",
    "    pass\n",
    "    ### BEGIN STRIP ###\n",
    "    return sc.textFile(filepath)\\\n",
    "    .flatMap(lambda line: line.split(' '))\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda t: -t[1])\n",
    "    ### END STRIP ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0595cb44-2529-4646-a628-05794ca5b250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Use `count_words` with `FILENAME` and check its type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770a1b71-3f67-4652-a5ac-81ab6742f9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[99]: pyspark.rdd.PipelinedRDD"
     ]
    }
   ],
   "source": [
    "rdd = count_words(FILENAME)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5bad4b7-6b2f-4c3e-99b0-f06d716b1826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It should be a `pyspark.rdd.PipelineRDD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26544f08-40ce-4ce4-bde8-c32f02c4d47f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Finally, take the 10 first elements of your RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ba8844-6e64-4e60-bc98-93babc9fba1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[100]: [('rain', 14),\n",
      " ('I', 14),\n",
      " ('you', 14),\n",
      " ('purple', 14),\n",
      " ('to', 13),\n",
      " ('', 10),\n",
      " ('Purple', 9),\n",
      " ('rain,', 9),\n",
      " ('only', 7),\n",
      " ('see', 6)]"
     ]
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6707df-01f8-401b-be34-7463c5d1c5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's it, you've done it!\n",
    "\n",
    "You've created a Spark job, the next step would be to neatly package this into a Python executable and submit it to a Spark Cluster for batch or stream execution, but this is beyond the content of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e7100db-e601-4c24-a87c-efb7a368f890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Going further\n",
    "\n",
    "We used a toy dataset, we suggest you try with a bigger one."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "J01_mercredi_midi_PySpark_Word_Count",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
