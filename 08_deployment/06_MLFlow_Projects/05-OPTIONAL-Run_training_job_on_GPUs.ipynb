{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![snap](https://media.giphy.com/media/xTiJ4cVWew0klLuY96/giphy.gif)\n",
    "\n",
    "# Run training jobs on GPU\n",
    "\n",
    "## What you will learn in this course 🧐🧐\n",
    "\n",
    "As your experience in Deep Learning grows, you might need to use some other tools than Google Colab to train your models. One reason is because Google Colab might not always be free and also because GPUs running on it are the cheapest, and slowest on the market. \n",
    "\n",
    "In this course, we want to show you a simple way to run your training jobs on VMs that have NVIDIA GPUs installed. We will be using AWS EC2 instances to do so. You will learn:\n",
    "\n",
    "* How to launch a specific EC2 AMI \n",
    "* Run Docker containers on GPUs \n",
    "* Run a remote training job on GPUs\n",
    "\n",
    "\n",
    "## **Step-1**: Choose Amazon EC2 AMI\n",
    "\n",
    "Let's start our tutorial. The first thing that you will need to do is to launch a specific EC2 AMI which is:\n",
    "\n",
    "* *Deep Learning AMI (Amazon Linux 2) Version 56.0 - ami-0afac37ebdacee753*\n",
    "\n",
    "![crack](https://full-stack-assets.s3.eu-west-3.amazonaws.com/Deployment/Deep_learning_AMI.png)\n",
    "\n",
    "> 👋 AMI means **Amazon Machine Image**. This is simply an EC2 instance with a lot of presets that you won't have to do yourself. In the above example, you have an AMI with NVIDIA Driver, Docker etc. preinstalled. This is extremely useful because it alleviates the pain of installing this yourself! \n",
    "\n",
    "Follow the normal setup process (described in the course *Introducrtion to EC2*), simply **make sure that you have SSH enabled in your security group**. \n",
    "\n",
    "## **Step-2**: Install `mlflow`\n",
    "\n",
    "Either you do it when setting up your EC2 in step *3. Configure Instance > User Data*, or you SSH into your instance and run: \n",
    "\n",
    "* `pip install mlflow` \n",
    "\n",
    "## **Step-3**: Run your project\n",
    "\n",
    "Now this is where the magic begins! To run your training job on GPU, you simply need to specify a few additionnal arguments in your `mlflow run` command. Here it is: \n",
    "\n",
    "```bash\n",
    "mlflow run GITHUB_URL\\\n",
    " -A gpus=all\\\n",
    " -A runtime=nvidia\\\n",
    "```\n",
    "\n",
    "Both arguments `-A gpus=all` and `-A runtime=nvidia` are the ones that specifies to Docker container that it needs to look for GPU in the host machine to run. \n",
    "\n",
    "Now enjoy the power of GPU! 😉\n",
    "\n",
    "![I've got the power](https://media.giphy.com/media/A9grgCQ0Dm012/giphy-downsized-large.gif)\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting \n",
    "\n",
    "There might be some bugs. Here are two lead to explore to fix them: \n",
    "\n",
    "1. Make sure that your instance has GPU already installed. \n",
    "\n",
    "2. Verify if Docker and GPUs work well by running:\n",
    "    * `sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi`\n",
    "\n",
    "You should see the following output:\n",
    "\n",
    "```bash\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
    "| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources 📚📚\n",
    "\n",
    "* [Install GPU with Docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-on-amazon-linux)\n",
    "* [Which EC2 instances has GPUs](https://aws.amazon.com/fr/ec2/instance-types/g4/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
