{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package training jobs ðŸ“¦\n",
    "\n",
    "\n",
    "## What you will learn in this course ðŸ§ðŸ§\n",
    "\n",
    "Now that you know a little more about MLFlow projects, you might wonder: *Why would we want to package ML models?*. This is definitely a good question. The answer is that packaging models solves a lot of problems. Among them are: \n",
    "\n",
    "1. No more - *I don't understand, it works on my machine* - Your model works everywhere \n",
    "2. You can easily run training jobs on more powerful remote machines to reduce training time significantly\n",
    "\n",
    "In this course, we will especially focus on the second aspect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote training ðŸ“º\n",
    "\n",
    "\n",
    "Just like `MLModel` file for standardizing our models, we will create a `MLProject` that is yet another `yaml` file that will describe all the configurations of our project. Our working directory will therefore contain the following files\n",
    "\n",
    "```shell \n",
    "â”œâ”€â”€ MLProject # Configuration of our training job\n",
    "â””â”€â”€ train.py # Our python file containing our training script\n",
    "```\n",
    "\n",
    "### Entry Point file ðŸšªðŸš¶\n",
    "\n",
    "`train.py` is called the entry point file. You need to have a script or an `.sh` file because notebooks are not accepted. Therefore you need to have your whole training process into your own file. The content look like this:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Set your variables for your environment\n",
    "    EXPERIMENT_NAME=\"Default\"\n",
    "\n",
    "    # Set experiment's info \n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    # Get our experiment info\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "    print(\"training model...\")\n",
    "    \n",
    "    # Time execution\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call mlflow autolog\n",
    "    mlflow.sklearn.autolog(log_models=False) # We won't log models right away\n",
    "\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_estimators\")\n",
    "    parser.add_argument(\"--min_samples_split\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Import dataset\n",
    "    df = pd.read_csv(\"https://julie-2-next-resources.s3.eu-west-3.amazonaws.com/full-stack-full-time/linear-regression-ft/californian-housing-market-ft/california_housing_market.csv\")\n",
    "\n",
    "    # X, y split \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # Train / test split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    # Pipeline \n",
    "    n_estimators = int(args.n_estimators)\n",
    "    min_samples_split=int(args.min_samples_split)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"standard_scaler\", StandardScaler()),\n",
    "        (\"Regressor\",RandomForestRegressor(n_estimators=n_estimators, min_samples_split=min_samples_split))\n",
    "    ])\n",
    "\n",
    "    # Log experiment to MLFlow\n",
    "\n",
    "    with mlflow.start_run(experiment_id = experiment.experiment_id):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_train)\n",
    "\n",
    "        # Log model seperately to have more flexibility on setup \n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"modeling_housing_market\",\n",
    "            registered_model_name=\"random_forest\",\n",
    "            signature=infer_signature(X_train, predictions)\n",
    "        )\n",
    "        \n",
    "    print(\"...Done!\")\n",
    "    print(f\"---Total training time: {time.time()-start_time}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MLProject`\n",
    "\n",
    "Now let's configure our `MLProject` file. We'll use Docker to standardize our environement that we will specify by writing:\n",
    "\n",
    "```yaml\n",
    "docker_env:\n",
    "  image: IMAGE_NAME\n",
    "```\n",
    "\n",
    "> If you haven't already, you will need to create an image with the required environment. In our case, we will be using `jedha/sample-mlflow-server` that we already created (you can use it as well if you don't want to bother building another image).\n",
    "\n",
    "Let's write the full file:\n",
    "\n",
    "```yaml\n",
    "# Name of the experiment (and the new docker image that MLFlow will create when running the file)\n",
    "name: californian_housing_market \n",
    "\n",
    "docker_env:\n",
    "  # Name of the image we'll be running a container from\n",
    "  image: sample-mlflow-server\n",
    "  # Volume binding\n",
    "  volumes: [\"$(pwd):/home/app\"]\n",
    "  # Set environment variables\n",
    "  environment: [\n",
    "      \"MLFLOW_TRACKING_URI\", \n",
    "      \"AWS_ACCESS_KEY_ID\",\n",
    "      \"AWS_SECRET_ACCESS_KEY\",\n",
    "      \"BACKEND_STORE_URI\",\n",
    "      \"ARTIFACT_ROOT\"\n",
    "    ]\n",
    "    \n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      # Parameter for our training\n",
    "      n_estimators: {type: int, default: 15} \n",
    "      # Parameter for our training\n",
    "      min_samples_split: {type: int, default: 3} \n",
    "    # Command that will be run when running that file \n",
    "    command: \"python train.py --n_estimators {n_estimators} --min_samples_split {min_samples_split}\" \n",
    "```\n",
    "\n",
    "\n",
    "As you can see in the `entry_points:` section we specified some `parameters` and provide the command `python train.py --n_estimators {n_estimators} --min_samples_split {min_samples_split}`. This is going to be useful when we will be running our training job. \n",
    "\n",
    "Also we have specified that there will be **environment variables**. We haven't given any values to it but `MLProject` will look for these variables when we will be running the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables \n",
    "\n",
    "Last piece of config: *environment variables*. Remember, environment variables are configuration that usually needs to be secret. **You should not share your environment variables to anybody but yourself** ðŸ”\n",
    "\n",
    "Here, we need to setup some variables. To do so, we will create a separate file that we will call `secrets.sh`. It will contain the following bash commands:\n",
    "\n",
    "```bash \n",
    "export MLFLOW_TRACKING_URI=\"REPLACE_ME_WITH_YOUR_MLFLOW_TRACKING_URI\"\n",
    "export AWS_ACCESS_KEY_ID=\"REPLACE_ME_WITH_YOUR_AWS_ACCESS_KEY_ID\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"REPLACE_ME_WITH_YOUR_AWS_SECRET_ACCESS_KEY\"\n",
    "export BACKEND_STORE_URI=\"REPLACE_ME_WITH_YOUR_BACKEND_STORE_URI\"\n",
    "export ARTIFACT_ROOT=\"REPLACE_ME_WITH_YOUR_ARTIFACT_ROOT\"\n",
    "```\n",
    "\n",
    "Once you are done, you will need to run this file by doing:\n",
    "\n",
    "* `source secrets.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a training process ðŸƒâ€â™€ï¸\n",
    "\n",
    "Alright, now that we have everything configured. \n",
    "\n",
    "```shell \n",
    "$ mlflow run path_to_your_project\n",
    "```\n",
    "\n",
    "> NB: if you are already in the project folder, you can simply do:<br />\n",
    "> ```shell \n",
    "> $ mlflow run . \n",
    "> ```\n",
    "\n",
    "To add parameters:\n",
    "```shell\n",
    "$ mlflow run path_to_your_project -P n_estimators=80\n",
    "```\n",
    "\n",
    "> ðŸ‘‹ Make sure that mlflow is installed on your computer (it might not be the case if you only worked in Docker containers) ðŸ‘‰ `pip install mlflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources ðŸ“šðŸ“š\n",
    "\n",
    "* [MLFlow Project](https://mlflow.org/docs/latest/projects.html#project-environments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
