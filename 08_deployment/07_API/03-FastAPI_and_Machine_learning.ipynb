{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI + Machine Learning = ðŸ’–\n",
    "\n",
    "## What you'll learn in this course ðŸ§ðŸ§\n",
    "\n",
    "FastAPI is a powerful tool to serve Machine Learning models. In this course, we will teach you: \n",
    "\n",
    "* How to build endpoints that serves ML models \n",
    "* Customize your code for batch predictions \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo App \n",
    "\n",
    "For this course, we will still be using this [Demo API](https://jedha-fast-api-demo.herokuapp.com/). Feel free to `git clone` [this repository](https://github.com/JedhaBootcamp/fast_api_demo_app) if you want to check out the source code. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve ML Models \n",
    "\n",
    "### **`mlflow`** power \n",
    "\n",
    "Let's combine the power of `mlflow` with FastAPI. Let's open up the URL of one of our model in our MLFLOW Tracking application:\n",
    "\n",
    "* [Salary Estimator](https://sample-mlflow-app.herokuapp.com/#/experiments/7)\n",
    "\n",
    "Click on the first experiment and you should see the *Artifacts* folder with some code example: \n",
    "\n",
    "![snap](https://full-stack-assets.s3.eu-west-3.amazonaws.com/Deployment/Mlflow_model_serving.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it requires only a few lines of code to start using your model. All you have to do is to load it using `mlflow` and this code: \n",
    "\n",
    "```python\n",
    "# Read data \n",
    "df = pd.DataFrame({\"some\": [\"data\"]})\n",
    "\n",
    "# Log model from mlflow \n",
    "logged_model = 'YOUR_MLFLOW_RUN_URI'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "prediction = loaded_model.predict(df)\n",
    "\n",
    "```\n",
    "And that's it! \n",
    "\n",
    "In our application, we used `mlflow` in FastAPI to predict salaries given a number of years of experience just like this: \n",
    "\n",
    "\n",
    "```python \n",
    "class PredictionFeatures(BaseModel):\n",
    "    YearsExperience: float\n",
    "\n",
    "#### SOME CODE ####\n",
    "###################\n",
    "\n",
    "@app.post(\"/predict\", tags=[\"Machine Learning\"])\n",
    "async def predict(predictionFeatures: PredictionFeatures):\n",
    "    \"\"\"\n",
    "    Prediction of salary for a given year of experience! \n",
    "    \"\"\"\n",
    "    # Read data \n",
    "    years_experience = pd.DataFrame({\"YearsExperience\": [predictionFeatures.YearsExperience]})\n",
    "\n",
    "    # Log model from mlflow \n",
    "    logged_model = 'runs:/323c3b4a6a6242b7837681bd5c539b27/salary_estimator'\n",
    "\n",
    "    # Load model as a PyFuncModel.\n",
    "    loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "    prediction = loaded_model.predict(years_experience)\n",
    "\n",
    "    # Format response\n",
    "    response = {\"prediction\": prediction.tolist()[0]}\n",
    "    return response\n",
    "```\n",
    "\n",
    "As you can see, we simply loaded the data (`predictionFeatures.YearsExperience`) as a `DataFrame` and then used our model logged in our app. \n",
    "\n",
    "\n",
    "> ðŸ‘‹ This code will **only work if your model is logged on Mlflow Registry**. Make sure that it's the case ðŸ˜‰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app \n",
    "\n",
    "If you want to run the app, simply use Docker as usual: \n",
    "\n",
    "```bash \n",
    "docker run -it \\\n",
    "-v \"$(pwd):/home/app\" \\\n",
    "-p 4000:4000 \\\n",
    "-e PORT=4000 \\\n",
    "-e MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI \\\n",
    "-e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n",
    "-e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n",
    "-e BACKEND_STORE_URI=$BACKEND_STORE_URI \\\n",
    "-e ARTIFACT_ROOT=$ARTIFACT_ROOT \\\n",
    "jedha/fast_api_demo_app\n",
    "```\n",
    "\n",
    "As usual, make sure you define your environment variables! \n",
    "\n",
    "> ðŸ‘‹ Your AWS credentials need to be authorized by Jedha, otherwise the API won't be able to access `mlflow`. If you don't have credentials, you will need to create your own model, with your own API in production and your own Docker image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch predictions \n",
    "\n",
    "The above code that we showed you **is great for one prediction** but what happens if you want to make a lot of predictions at once? Well this is what is called **batch predictions**. To do so you will need to upload a file (*i.e* `.csv` or `.xlsx`) and then make predictions. \n",
    "\n",
    "### Upload files with FastAPI \n",
    "\n",
    "First, let's learn how to upload files using FastAPI. To do so, you will need to import `File` and `UploadFile` from `fastapi`: \n",
    "\n",
    "```python \n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "### SOME CODE### \n",
    "```\n",
    "\n",
    "Then simply create a **`POST`** request that will use this file like this:\n",
    "\n",
    "```python \n",
    "@app.post(\"/post-picture\", tags=[\"Blog Endpoints\"])\n",
    "async def post_picture(file: UploadFile= File(...)):\n",
    "    \"\"\"\n",
    "    Upload a picture and read its file name.\n",
    "    \"\"\"\n",
    "    return {\"picture\": file.filename}\n",
    "```\n",
    "\n",
    "Here we simply upload a file and read its name using the `file.filename` attribute. \n",
    "\n",
    "\n",
    "### Batch predictions with files \n",
    "\n",
    "Now `file.filename` attribute isn't really useful. However, you can use `file.file` to read the actual content of the file and therefore load it in a `pandas` DataFrame. \n",
    "\n",
    "```python \n",
    "@app.post(\"/batch-pred\")\n",
    "async def batch_pred(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Make batch predictions \n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file.file)\n",
    "\n",
    "    # Log model from mlflow \n",
    "    logged_model = 'runs:/5e54b2ee620546b0914c9e9fbfd18875/salary_estimator'\n",
    "\n",
    "    # Load model as a PyFuncModel.\n",
    "    loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "    prediction = loaded_model.predict(df)\n",
    "\n",
    "    # Format response\n",
    "    response = {\"prediction\": prediction.tolist()}\n",
    "\n",
    "    return response\n",
    "```\n",
    "\n",
    "And voilÃ ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources ðŸ“šðŸ“š\n",
    "\n",
    "* [Request Body](https://fastapi.tiangolo.com/tutorial/body/)\n",
    "* [Request Files](https://fastapi.tiangolo.com/tutorial/request-files/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
